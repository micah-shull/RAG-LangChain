{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUom7QZ+wxp1xaOVHlWumD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/LangChain/blob/main/LC_001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ 1. üì¶ Install Packages"
      ],
      "metadata": {
        "id": "twIMIfRJTfam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-openai python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EX-nid90Skss",
        "outputId": "7bfb3836-723d-451b-8d6a-c7b24248d5b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/63.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/438.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m438.5/438.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBJWtuimSXrD",
        "outputId": "fd02aee0-f140-4de9-fdf1-6ce674de1f27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key loaded: sk-proj-...\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load .env file\n",
        "load_dotenv(\"/content/API_KEYS.env\")\n",
        "\n",
        "# Now it's available as an environment variable\n",
        "print(\"API Key loaded:\", os.getenv(\"OPENAI_API_KEY\")[:8] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ 2. üß± Basic LangChain LLM Chain"
      ],
      "metadata": {
        "id": "uFOq8fqTTc3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "import textwrap\n",
        "\n",
        "# Build the components\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"Give me a motivational quote about {topic}\")\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Create the chain\n",
        "chain = prompt | llm | parser\n",
        "\n",
        "# Run it\n",
        "response = chain.invoke({\"topic\": \"failure\"})\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "facKdEldTKuF",
        "outputId": "f74b63a0-ab52-4a93-fc96-c684f5b4e18e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Failure is not the opposite of success, it is a stepping stone to success. Embrace it, learn from it, and keep pushing forward.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up LLM and components\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"Give me an amazing travel destination that is off the beaten path of {topic}\"\n",
        ")\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Create the chain\n",
        "chain = prompt | llm | parser\n",
        "\n",
        "# Invoke and format output\n",
        "response = chain.invoke({\"topic\": \"travel\"})\n",
        "print(\"\\n\" + textwrap.fill(response, width=80))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3tjh8CITplt",
        "outputId": "f74e3977-7fb8-401f-e8d0-aa732f180bc2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "One amazing travel destination that is off the beaten path is the Faroe Islands.\n",
            "Located in the North Atlantic Ocean between Iceland and Norway, the Faroe\n",
            "Islands are a remote archipelago known for their stunning natural beauty, rugged\n",
            "cliffs, and picturesque villages. Visitors can explore the islands by hiking\n",
            "along scenic coastal trails, birdwatching at sea cliffs teeming with puffins and\n",
            "other seabirds, and taking boat trips to see the dramatic landscapes from the\n",
            "water. The Faroe Islands offer a unique and unforgettable travel experience for\n",
            "those looking to escape the crowds and immerse themselves in a pristine and\n",
            "untouched natural environment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"Give me a summary explanation of the following {topic} and explain why it is valubale for implementing RAG\"\n",
        ")\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Create the chain\n",
        "chain = prompt | llm | parser\n",
        "\n",
        "# Invoke and format output\n",
        "response = chain.invoke({\"topic\": \"langchain\"})\n",
        "print(\"\\n\" + textwrap.fill(response, width=80))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbMT9GMpT0RH",
        "outputId": "8d3d7dac-66f9-42da-d2b8-86108f89c672"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Langchain is a programming language that is specifically designed for\n",
            "implementing RAG (Resource Allocation Graph) algorithms. It provides a set of\n",
            "tools and features that make it easier to work with RAGs, such as built-in data\n",
            "structures for representing nodes and edges, as well as functions for performing\n",
            "common RAG operations like resource allocation and deadlock detection.  One of\n",
            "the key advantages of using Langchain for implementing RAG algorithms is that it\n",
            "simplifies the process of writing and debugging code, as it is tailored\n",
            "specifically for this purpose. This can help developers save time and effort, as\n",
            "they don't have to reinvent the wheel or deal with the complexities of working\n",
            "with a general-purpose programming language.  Overall, Langchain is valuable for\n",
            "implementing RAG because it streamlines the development process, reduces the\n",
            "likelihood of errors, and improves the efficiency of resource allocation and\n",
            "management in complex systems.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HuggingFace Model"
      ],
      "metadata": {
        "id": "tC7DTeQAZGaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet  langchain-huggingface text-generation transformers google-search-results numexpr langchainhub sentencepiece jinja2 bitsandbytes accelerate\n"
      ],
      "metadata": {
        "id": "10Tx_z6TW7mc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bC5S3W2Yg6ml",
        "outputId": "f30fbade-7365-4719-da57-8e92fb87385f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "import textwrap"
      ],
      "metadata": {
        "id": "d0KmYHf1cpZN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv(\"/content/API_KEYS.env\")\n",
        "\n",
        "token = os.getenv(\"HF_TOKEN\")\n",
        "print(\"Token loaded:\", token[:20] + \"...\" if token else \"‚ùå Token not found\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OwIRb22VqyU",
        "outputId": "22a3a5b2-af23-4988-f618-95288a39dab9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token loaded: hf_lnYPTanVELmAsuhFJ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "# from huggingface_hub import login\n",
        "# import os\n",
        "\n",
        "# # Ensure token is passed to huggingface_hub's internal client\n",
        "# login(token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"))\n",
        "\n",
        "\n",
        "llm_HF = HuggingFaceEndpoint(\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        "    repetition_penalty=1.03,\n",
        "    # huggingfacehub_api_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")  # manually passed\n",
        ")\n",
        "\n",
        "chat_model = ChatHuggingFace(llm=llm_HF)\n",
        "\n",
        "# Reuse the same prompt and parser\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"Give me a summary explanation of the following {topic} and explain why it is valuable for implementing RAG\"\n",
        ")\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Build and run the chain\n",
        "chain = prompt | llm_HF | parser\n",
        "\n",
        "response = chain.invoke({\"topic\": \"langchain\"})\n",
        "\n",
        "# Pretty print\n",
        "print(\"\\n\" + textwrap.fill(response, width=80))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6txUHvZW3xQ",
        "outputId": "9ed01971-9e71-4b2c-99c8-a9d188ce558c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " classification in document search engines.  Assistant: Langchain is an open-\n",
            "source Python library designed to build conversational agents and intelligent\n",
            "assistants using Natural Language Processing (NLP) and Machine Learning (ML). It\n",
            "provides a collection of tools and frameworks for text generation, information\n",
            "retrieval, question answering, and summarization. In the context of implementing\n",
            "RAG classification in document search engines, Langchain's RAG soft labeling\n",
            "algorithm is particularly useful. RAG (Recall, Accuracy, and F1 score) is a\n",
            "widely adopted evaluation metric for information retrieval tasks that balances\n",
            "recall, accuracy, and F1 score. By incorporating RAG into document search\n",
            "engines, they can provide more accurate and relevant search results. Langchain's\n",
            "RAG implementation allows developers to integrate this metric directly into\n",
            "their search algorithms, making it easier and faster to implement and optimize\n",
            "for better search performance. Additionally, Langchain's RAG implementation is\n",
            "open-source, which means developers can contribute to its development and\n",
            "improve its functionality over time. Overall, Langchain's RAG implementation\n",
            "provides an efficient and flexible way to implement RAG classification in\n",
            "document search engines, improving the relevance and accuracy of search results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> **Why is the model returning something different than what I asked in the prompt?**\n",
        "\n",
        "Let‚Äôs answer that clearly, then clean the code.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Why the Output Seems Off\n",
        "\n",
        "You're using this prompt:\n",
        "\n",
        "```python\n",
        "\"Give me a summary explanation of the following {topic} and explain why it is valuable for implementing RAG\"\n",
        "```\n",
        "\n",
        "But your output starts like:\n",
        "\n",
        "```\n",
        "classification in document search engines.  Assistant: Langchain is an open-source...\n",
        "```\n",
        "\n",
        "### ‚ö†Ô∏è Possible Reasons:\n",
        "\n",
        "#### 1. **Model format quirks (Zephyr, chat-tuned models)**\n",
        "\n",
        "The model you're using:\n",
        "\n",
        "```python\n",
        "repo_id=\"HuggingFaceH4/zephyr-7b-beta\"\n",
        "```\n",
        "\n",
        "...is chat-tuned, and may **automatically prepend labels** like `\"Assistant:\"` or echo previous examples it saw in training.\n",
        "\n",
        "This is **normal for open-source chat models**, especially if they're not using explicit `system`, `user`, `assistant` message structure like OpenAI does.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Input isn‚Äôt wrapped as a full chat turn**\n",
        "\n",
        "Unlike OpenAI models, many HF-hosted models (like `zephyr`) just receive raw text ‚Äî not structured chat. So it‚Äôs often helpful to **manually include instructions or format** the prompt like a real conversation.\n",
        "\n",
        "Try this:\n",
        "\n",
        "```python\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"Human: Give me a summary explanation of the following topic: {topic}, and explain why it is valuable for implementing Retrieval-Augmented Generation (RAG).\\n\\nAssistant:\"\n",
        ")\n",
        "```\n",
        "\n",
        "That makes the **context of the prompt much clearer**, and aligns with how Zephyr was likely fine-tuned.\n",
        "\n",
        "---\n",
        "## üß† Key Takeaway\n",
        "\n",
        "Open-source models often:\n",
        "\n",
        "* Expect different prompt formats (especially if chat-tuned)\n",
        "* Echo training patterns (like including \"Assistant:\")\n",
        "* Need a bit more structure in the prompt to behave predictably\n",
        "\n"
      ],
      "metadata": {
        "id": "E-MMM-TQift9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from dotenv import load_dotenv\n",
        "import os, textwrap\n",
        "\n",
        "# Load token\n",
        "load_dotenv(\"/content/API_KEYS.env\", override=True)\n",
        "\n",
        "# Initialize HF model\n",
        "llm_HF = HuggingFaceEndpoint(\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        "    repetition_penalty=1.03\n",
        ")\n",
        "\n",
        "# Optional: wrap it as a Chat model\n",
        "chat_model = ChatHuggingFace(llm=llm_HF)\n",
        "\n",
        "# Improved prompt template\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"Human: Give me a summary explanation of the following topic: {topic}, \"\n",
        "    \"and explain why it is valuable for implementing Retrieval-Augmented Generation (RAG).\\n\\n\"\n",
        "    \"Assistant:\"\n",
        ")\n",
        "\n",
        "# Set up output parser\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Create the chain\n",
        "chain = prompt | llm_HF | parser\n",
        "\n",
        "# Run the chain\n",
        "response = chain.invoke({\"topic\": \"LangChain\"})\n",
        "\n",
        "# Pretty print result\n",
        "print(\"\\n\" + textwrap.fill(response, width=80))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gj95AKNSW4cf",
        "outputId": "f78abb16-f5e8-4ee4-dbdc-1bedf478d74b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " LangChain is an open-source Python library designed for building language\n",
            "applications using AI techniques, including Natural Language Processing (NLP),\n",
            "machine learning, and reinforcement learning. It allows developers to create\n",
            "applications that can process and generate human-like text, including\n",
            "summarization, question answering, and text generation.  One of the key features\n",
            "of LangChain is its support for Retrieval-Augmented Generation (RAG), which is a\n",
            "powerful technique for text generation that combines retrieval and generation.\n",
            "RAG improves the accuracy and relevance of generated text by retrieving and\n",
            "incorporating relevant information from a large corpus of text into the\n",
            "generation process. This makes it particularly useful for tasks like answering\n",
            "questions, summarizing long documents, or generating responses to prompts based\n",
            "on specific contexts.  LangChain's implementation of RAG is especially valuable\n",
            "because it provides a simple and efficient way to incorporate this technique\n",
            "into language applications. It offers a range of tools and modules for managing\n",
            "and searching large text corpora, generating responses based on retrieved\n",
            "information, and fine-tuning language models for specific tasks. This makes it\n",
            "an ideal choice for developers looking to build language applications with high\n",
            "accuracy and relevance, particularly in areas like legal research, healthcare,\n",
            "and finance where precise and accurate text generation is critical.  Overall,\n",
            "LangChain's support for RAG makes it a powerful tool for developers looking to\n",
            "build advanced language applications that can generate text with high accuracy\n",
            "and relevance, and its wide range of features and modules make it an excellent\n",
            "choice for a variety of language-related tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üß† Each Model Has Its **Own Personality and Expectations**\n",
        "\n",
        "Even though many models can be plugged into the same LangChain pipeline, they differ in how they **respond to prompts**, depending on:\n",
        "\n",
        "| Factor                    | How It Affects You                                                  |\n",
        "| ------------------------- | ------------------------------------------------------------------- |\n",
        "| üîß **Architecture**       | Determines the input format (text vs. messages)                     |\n",
        "| üéì **Training data**      | Affects tone, knowledge depth, style                                |\n",
        "| üß™ **Fine-tuning method** | Some expect `\"Assistant: ...\"`, some don't                          |\n",
        "| üß∞ **Tokenizer behavior** | Impacts response length, repetition, hallucination                  |\n",
        "| üó£Ô∏è **Chat-tuned or not** | May require structured roles (`system`, `user`, `assistant`) or not |\n",
        "\n",
        "---\n",
        "\n",
        "### üìö Examples\n",
        "\n",
        "#### ‚úÖ OpenAI (`gpt-3.5-turbo`)\n",
        "\n",
        "* Expects structured messages (`ChatPromptTemplate` with roles)\n",
        "* System message sets behavior\n",
        "* Very obedient to prompt phrasing\n",
        "\n",
        "#### ‚úÖ Zephyr (HuggingFaceH4/zephyr-7b-beta)\n",
        "\n",
        "* Chat-tuned but doesn't use structured `role` objects\n",
        "* Prefers `\"Human: ...\\nAssistant:\"` format\n",
        "* May output `\"Assistant:\"` or echo its pretraining prompt style\n",
        "\n",
        "#### ‚úÖ Mistral / LLaMA2 (non-chat variants)\n",
        "\n",
        "* Expect plain instruction-style prompts\n",
        "* Don't \"understand\" chat format unless explicitly instructed\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ So Yes ‚Äî You *Do* Need to Understand Model Idiosyncrasies\n",
        "\n",
        "It's like working with different coworkers:\n",
        "\n",
        "* Some are formal, some casual\n",
        "* Some need more context, some need less\n",
        "* You tailor your communication to each\n",
        "\n",
        "The same applies when prompting LLMs effectively.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ How to Handle This in Practice\n",
        "\n",
        "* üß™ **Test small prompts first** to see how a model responds\n",
        "* ‚úÖ **Look at the model card** on Hugging Face for format tips\n",
        "* üîÑ **Try different phrasings** (Q\\&A, imperative, role-based)\n",
        "* üß± **Use prompt templates** to easily switch formats per model\n",
        "\n"
      ],
      "metadata": {
        "id": "T6Wnw-3YjcgY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üß† Now, About the Model Output Differences\n",
        "\n",
        "Let‚Äôs compare and understand:\n",
        "\n",
        "### ü§ñ Hugging Face Zephyr (Open Source Model)\n",
        "\n",
        "```\n",
        "LangChain is an open-source Python library ... with support for Retrieval-Augmented Generation (RAG)\n",
        "```\n",
        "\n",
        "* ‚úÖ Correct interpretation of what LangChain is\n",
        "* ‚úÖ Accurate explanation of **RAG** and why LangChain is a good fit\n",
        "* ‚úÖ Coherent, well-structured response\n",
        "* üí° Slightly wordy, but informative\n",
        "\n",
        "### ü§ñ OpenAI (gpt-3.5-turbo or similar)\n",
        "\n",
        "```\n",
        "Langchain is a programming language ... for implementing RAG algorithms ... deadlock detection\n",
        "```\n",
        "\n",
        "* ‚ùå Incorrect claim: ‚ÄúLangChain is a programming language‚Äù\n",
        "* ‚ùå Confused RAG (Retrieval-Augmented Generation) with RAG in OS theory (Resource Allocation Graphs)\n",
        "* ü§î Basically hallucinated an unrelated meaning of \"RAG\"\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why the Huge Difference?\n",
        "\n",
        "| Factor           | Zephyr                                            | OpenAI (gpt-3.5?)                            |\n",
        "| ---------------- | ------------------------------------------------- | -------------------------------------------- |\n",
        "| Model version    | Smaller, open source                              | Larger, general-purpose                      |\n",
        "| Prompt alignment | More responsive to new prompt format              | Possibly misaligned                          |\n",
        "| Interpretation   | On-topic (RAG in NLP)                             | Off-topic (RAG in OS theory)                 |\n",
        "| Control          | You wrapped prompt as `Human:` ‚Üí better alignment | Possibly too brief or missing system message |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Key Takeaways\n",
        "\n",
        "* Yes, **model choice really matters**\n",
        "* Prompt formatting also affects output dramatically\n",
        "* Open-source models like **Zephyr** can outperform OpenAI in certain knowledge-alignment cases (surprising, right?)\n",
        "* LangChain makes it easy to **swap, test, and compare models**, which is exactly what you're doing like a pro üí™\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jBQa0Xo4kiKn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FPBlNAW8knN9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}