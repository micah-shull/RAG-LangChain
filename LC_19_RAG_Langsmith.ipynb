{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMZJ5ERI+cXGmFR8u0yZA/o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/RAG-LangChain/blob/main/LC_19_RAG_Langsmith.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ✅ **`%pip` vs. `%%pip` vs. `!pip`**\n",
        "\n",
        "1️⃣ **`!pip install ...`**\n",
        "\n",
        "* This runs `pip` in a **shell subprocess** (like typing in a terminal).\n",
        "* In Colab/Jupyter, this can sometimes install packages into a different Python environment than the one the notebook kernel uses — especially if multiple Python versions or virtual environments are involved.\n",
        "* You might install a package but then `import` still fails → annoying!\n",
        "\n",
        "2️⃣ **`%pip install ...`**\n",
        "\n",
        "* `%pip` is an **IPython magic command** (single-line).\n",
        "* It ensures that the package is installed in **the same Python environment as your notebook kernel**.\n",
        "* It’s the recommended way for pip installs in Jupyter/Colab.\n",
        "\n",
        "✅ Use `%pip` instead of `!pip` to avoid “module not found” surprises.\n",
        "\n",
        "3️⃣ **`%%pip install ...`**\n",
        "\n",
        "* `%%pip` is the **cell magic** version of `%pip`.\n",
        "* It works exactly the same way but applies to the **entire cell**.\n",
        "* Useful if you have multiple install lines in one cell.\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡ **Example**\n",
        "\n",
        "```python\n",
        "# Good practice in Google Colab\n",
        "%pip install langchain-openai langsmith\n",
        "\n",
        "# Or, using cell magic:\n",
        "%%pip install langchain-openai langsmith\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🗂️ **What about `%%capture`?**\n",
        "\n",
        "You wrote:\n",
        "\n",
        "```python\n",
        "%%capture --no-stderr\n",
        "%pip install ...\n",
        "```\n",
        "\n",
        "* `%%capture` is another IPython cell magic that **captures stdout and stderr** (so you don’t see all the pip output).\n",
        "* `--no-stderr` means it won’t capture errors → you’ll still see them if pip fails.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Best practice for Colab**\n",
        "\n",
        "* Use `%pip` or `%%pip` instead of `!pip` for Python packages.\n",
        "* Use `%%capture` if you want to hide noisy output.\n",
        "* Always restart your runtime (`Runtime → Restart runtime`) after major installs, if needed.\n",
        "\n"
      ],
      "metadata": {
        "id": "2db_kAIwGQdv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VaCkH33pECeN"
      },
      "outputs": [],
      "source": [
        "# %%capture --no-stderr\n",
        "# %pip install langsmith langchain-openai langchain-core langchain-community pydantic python-dotenv openai langgraph\n",
        "# %pip install --upgrade langsmith"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -Uq langsmith langchain-openai langchain-core langchain-community pydantic python-dotenv openai langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v47INGDfGWDq",
        "outputId": "df5bca19-0638-4fe8-d415-7847af55bb54"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.4/441.4 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.1/755.1 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.7/143.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environ Setup"
      ],
      "metadata": {
        "id": "ET_-WfA5Ja74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1) Imports ---\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# --- 2) Load environment variables ---\n",
        "load_dotenv(\"/content/API_KEYS.env\", override=True)\n",
        "\n",
        "# Confirm keys\n",
        "print(\"OPENAI_API_KEY:\", os.getenv(\"OPENAI_API_KEY\"))\n",
        "print(\"LANGCHAIN_API_KEY:\", os.getenv(\"LANGCHAIN_API_KEY\"))\n",
        "print(\"LANGCHAIN_PROJECT:\", os.getenv(\"LANGCHAIN_PROJECT\"))\n",
        "print(\"LANGCHAIN_TRACING_V2:\", os.getenv(\"LANGCHAIN_TRACING_V2\"))\n",
        "\n",
        "# Alternatively set project name for this run outside .env file\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"project_00\"\n",
        "os.environ[\"USER_AGENT\"] = \"MyRAGApp/0.1\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMRJP96sEVGe",
        "outputId": "84612ad4-294d-495c-da14-a5e76f36f3a9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OPENAI_API_KEY: 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\n",
            "LANGCHAIN_API_KEY: None\n",
            "LANGCHAIN_PROJECT: my_project_name\n",
            "LANGCHAIN_TRACING_V2: true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Selection"
      ],
      "metadata": {
        "id": "E9GZhgEEJYoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
      ],
      "metadata": {
        "id": "kuQ_Y4HtJXH2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings\n",
        "\n",
        "✅ Perfect — you’re doing it exactly right for embeddings!\n",
        "Let me clarify how this works so you know you’re good to go:\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Using `OpenAIEmbeddings`**\n",
        "\n",
        "```python\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "```\n",
        "\n",
        "**What’s happening?**\n",
        "\n",
        "* `OpenAIEmbeddings` is the LangChain wrapper that calls OpenAI’s embedding endpoint.\n",
        "* `model=\"text-embedding-3-large\"` is the recommended newer embedding model (high quality).\n",
        "* Your `OPENAI_API_KEY` must be set (which you have in your `.env`).\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 **How it works**\n",
        "\n",
        "When you call:\n",
        "\n",
        "```python\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "```\n",
        "\n",
        "or:\n",
        "\n",
        "```python\n",
        "embeddings.embed_query(\"My question?\")\n",
        "```\n",
        "\n",
        "LangChain calls OpenAI’s embedding endpoint, gets the vector, and returns it to your retriever or vector store.\n"
      ],
      "metadata": {
        "id": "k3HwvlY0KLIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "\n",
        "# ⚡️ Tip: inspect embedding\n",
        "vector = embeddings.embed_query(\"How does LangSmith tracing work?\")\n",
        "print(len(vector))  # Should match the embedding dimension"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgB4R-KNKIUX",
        "outputId": "7ea85ec6-a659-4863-dbd0-c915d865538c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector Store"
      ],
      "metadata": {
        "id": "hiVLCWhKJ9iX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "vector_store = InMemoryVectorStore(embeddings)"
      ],
      "metadata": {
        "id": "bI2jUqIXJ8K-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import List, TypedDict\n",
        "\n",
        "# Load and chunk contents of the blog\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# Index chunks\n",
        "_ = vector_store.add_documents(documents=all_splits)\n",
        "\n",
        "# Define prompt for question-answering\n",
        "# N.B. for non-US LangSmith endpoints, you may need to specify\n",
        "# api_url=\"https://api.smith.langchain.com\" in hub.pull.\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "\n",
        "# Define state for application\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    answer: str\n",
        "\n",
        "\n",
        "# Define application steps\n",
        "def retrieve(state: State):\n",
        "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "\n",
        "def generate(state: State):\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"answer\": response.content}\n",
        "\n",
        "\n",
        "# Compile application and test\n",
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "YLD5DiFLLcli"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Breakdown\n",
        "\n",
        "Let’s break this block down step-by-step so you really see **what’s happening and why** — because this is a **perfect small RAG + LangGraph example** and there’s a lot to learn here! 🚀\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **🔹 1) Imports**\n",
        "\n",
        "```python\n",
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import List, TypedDict\n",
        "```\n",
        "\n",
        "* `bs4` → BeautifulSoup: you’ll use it to parse the HTML blog post.\n",
        "* `hub` → LangChain Hub: stores reusable prompts/templates.\n",
        "* `WebBaseLoader` → loads text from a webpage.\n",
        "* `Document` → LangChain’s document wrapper (metadata, page content).\n",
        "* `RecursiveCharacterTextSplitter` → splits long docs into chunks.\n",
        "* `StateGraph` → the new LCEL-based graph execution framework.\n",
        "* `TypedDict` → defines your app state type for type safety.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **🔹 2) Load and parse blog post**\n",
        "\n",
        "```python\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "```\n",
        "\n",
        "👉 This:\n",
        "\n",
        "* Loads **Lilian Weng’s “Agent” blog post**.\n",
        "* Uses `bs4.SoupStrainer` to keep only useful HTML parts (post content, title, header) → more focused chunks.\n",
        "* Wraps the text as `Document` objects.\n",
        "\n",
        "✅ **Why?** You don’t want your embeddings polluted by navbars, footers, or ads!\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **🔹 3) Split into chunks**\n",
        "\n",
        "```python\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "```\n",
        "\n",
        "👉 This:\n",
        "\n",
        "* Breaks long text into smaller, overlapping chunks (RAG best practice).\n",
        "* `chunk_size=1000`: each chunk \\~1000 characters.\n",
        "* `chunk_overlap=200`: 200 chars overlap between chunks to preserve context.\n",
        "\n",
        "✅ **Why?** Smaller chunks = better similarity search and fewer tokens sent to LLM.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **🔹 4) Index chunks**\n",
        "\n",
        "```python\n",
        "_ = vector_store.add_documents(documents=all_splits)\n",
        "```\n",
        "\n",
        "👉 This:\n",
        "\n",
        "* Adds the chunks to your vector store (`InMemoryVectorStore` or `FAISS`).\n",
        "* Each chunk is embedded and stored for similarity search later.\n",
        "\n",
        "✅ **Why?** This builds your local knowledge base to support retrieval.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **🔹 5) Pull RAG prompt from LangChain Hub**\n",
        "\n",
        "```python\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "```\n",
        "\n",
        "👉 This:\n",
        "\n",
        "* Loads a reusable RAG-style prompt from the LangChain Hub.\n",
        "* The prompt will look like:\n",
        "\n",
        "  ```\n",
        "  Use the following context to answer the question.\n",
        "  Question: {question}\n",
        "  Context: {context}\n",
        "  Answer:\n",
        "  ```\n",
        "\n",
        "✅ **Why?** Standardizes your RAG behavior without hardcoding your own prompt.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **🔹 6) Define app state**\n",
        "\n",
        "```python\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    answer: str\n",
        "```\n",
        "\n",
        "👉 This:\n",
        "\n",
        "* Defines the **input, intermediate, and output state** for your graph.\n",
        "* `question` → the user’s query.\n",
        "* `context` → retrieved docs.\n",
        "* `answer` → final LLM output.\n",
        "\n",
        "✅ **Why?** `StateGraph` needs a clear state schema to pass data between steps.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **🔹 7) Define steps**\n",
        "\n",
        "### `retrieve`\n",
        "\n",
        "```python\n",
        "def retrieve(state: State):\n",
        "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
        "    return {\"context\": retrieved_docs}\n",
        "```\n",
        "\n",
        "👉 This:\n",
        "\n",
        "* Takes the question.\n",
        "* Uses similarity search to find relevant chunks.\n",
        "* Returns them as `context` for the next step.\n",
        "\n",
        "✅ **Why?** Classic RAG step: “given a question, find matching chunks.”\n",
        "\n",
        "---\n",
        "\n",
        "### `generate`\n",
        "\n",
        "```python\n",
        "def generate(state: State):\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"answer\": response.content}\n",
        "```\n",
        "\n",
        "👉 This:\n",
        "\n",
        "* Joins retrieved chunks into a single context block.\n",
        "* Calls the prompt to format the input for the LLM.\n",
        "* Calls the LLM to generate the answer.\n",
        "* Returns the answer as your final state.\n",
        "\n",
        "✅ **Why?** This is the generation half of RAG: “use the retrieved context to produce a grounded answer.”\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **🔹 8) Build & compile your `StateGraph`**\n",
        "\n",
        "```python\n",
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "graph = graph_builder.compile()\n",
        "```\n",
        "\n",
        "👉 This:\n",
        "\n",
        "* Creates a graph where:\n",
        "\n",
        "  * **Start** → `retrieve` → `generate`\n",
        "* `StateGraph` wires up your pipeline with a clear flow.\n",
        "\n",
        "✅ **Why?** This modular graph pattern is reusable, composable, and debuggable — much better than messy nested calls!\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **🔹 9) Next step: Run it**\n",
        "\n",
        "After this block, you run:\n",
        "\n",
        "```python\n",
        "result = graph.invoke({\"question\": \"What are ReAct agents?\"})\n",
        "print(result)\n",
        "```\n",
        "\n",
        "* This runs `retrieve` → `generate` → gives you an answer.\n",
        "* ✅ And because your env vars are set, the entire run logs to **LangSmith**:\n",
        "\n",
        "  * Question\n",
        "  * Retrieved docs\n",
        "  * LLM output\n",
        "  * Token usage\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **📌 What this block gives you**\n",
        "\n",
        "✔️ A **clean RAG pipeline** with:\n",
        "\n",
        "* **Data loading**\n",
        "* **Chunking**\n",
        "* **Embeddings + vector store**\n",
        "* **Retrieval**\n",
        "* **Prompt templating**\n",
        "* **LLM generation**\n",
        "* **Modular graph orchestration**\n",
        "\n",
        "✔️ **Full observability** via LangSmith (if your API key & project are correct).\n",
        "\n",
        "---\n",
        "\n",
        "## 🎉 **Why this is powerful**\n",
        "\n",
        "* You can swap the retriever, LLM, or prompt independently.\n",
        "* You can add more steps — like evals, re-ranking, or feedback loops.\n",
        "* You can see the entire flow in your LangSmith dashboard to debug or tune.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-JSPckvzOA9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Soup Strainer"
      ],
      "metadata": {
        "id": "YIcN5L7uOrOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect raw HTML (not using SoupStrainer)\n",
        "import requests\n",
        "html = requests.get(\"https://lilianweng.github.io/posts/2023-06-23-agent/\").text\n",
        "\n",
        "print(html[:1000])  # Show first 1000 chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MbsZMS_OzML",
        "outputId": "0a96d616-28a2-43ef-d9e7-831a829b4848"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!DOCTYPE html>\n",
            "<html lang=\"en\" dir=\"auto\">\n",
            "\n",
            "<head><meta charset=\"utf-8\">\n",
            "<meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n",
            "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\">\n",
            "<meta name=\"robots\" content=\"index, follow\">\n",
            "<title>LLM Powered Autonomous Agents | Lil&#39;Log</title>\n",
            "<meta name=\"keywords\" content=\"nlp, language-model, agent, steerability, prompting\" />\n",
            "<meta name=\"description\" content=\"Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
            "Agent System Overview\n",
            "In a LLM-powered autonomous agent system, LLM functions as the agent&rsquo;s brain, complemented by several key components:\n",
            "\n",
            "Planning\n",
            "\n",
            "Subgoal and decomposition: The agent breaks down lar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(docs))\n",
        "print(docs[0].page_content[:500])  # show a snippet\n",
        "print(docs[0].metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zZkvdUUJW_l",
        "outputId": "419ff83c-7de2-43b4-c274-60e94a9dcc8d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "\n",
            "\n",
            "      LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
            "Agent System Overview#\n",
            "In\n",
            "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup, SoupStrainer\n",
        "\n",
        "# Define what you want to extract:\n",
        "only_useful = SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))\n",
        "\n",
        "# Parse HTML, filtering at parse time:\n",
        "soup = BeautifulSoup(html, \"html.parser\", parse_only=only_useful)\n",
        "\n",
        "# Check what you got:\n",
        "print(soup.prettify()[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVNBYuMsJXB_",
        "outputId": "c0687528-725a-417d-ed56-445f1e686fbd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<header class=\"post-header\">\n",
            " <h1 class=\"post-title\">\n",
            "  LLM Powered Autonomous Agents\n",
            " </h1>\n",
            " <div class=\"post-meta\">\n",
            "  Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            " </div>\n",
            "</header>\n",
            "<div class=\"post-content\">\n",
            " <p>\n",
            "  Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as\n",
            "  <a href=\"https://github.com/Significant-Gravitas/Auto-GPT\">\n",
            "   AutoGPT\n",
            "  </a>\n",
            "  ,\n",
            "  <a href=\"https://github.com/AntonOsika/gpt-engineer\">\n",
            "   GPT-Engineer\n",
            "  </a>\n",
            "  and\n",
            "  <a href=\"https://github.com/yoheinakajima/babyagi\">\n",
            "   BabyAGI\n",
            "  </a>\n",
            "  , serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
            " </p>\n",
            " <h1 id=\"agent-system-overview\">\n",
            "  Agent System Overview\n",
            "  <a aria-hidden=\"true\" class=\"anchor\" hidden=\"\" href=\"#agent-system-overview\">\n",
            "   #\n",
            "  </a>\n",
            " </h1>\n",
            " <p>\n",
            "  In a LLM-powered au\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup, SoupStrainer\n",
        "\n",
        "url = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
        "\n",
        "# Step 1: Get HTML\n",
        "html = requests.get(url).text\n",
        "\n",
        "# Step 2: Strain for useful parts\n",
        "only_useful = SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))\n",
        "soup = BeautifulSoup(html, \"html.parser\", parse_only=only_useful)\n",
        "\n",
        "# Step 3: Inspect\n",
        "print(\"EXTRACTED PARTS:\")\n",
        "extracted_parts = soup.find_all(True)\n",
        "\n",
        "for idx, part in enumerate(extracted_parts[0:15]):\n",
        "    print(f\"\\n--- PART {idx+1} ---\")\n",
        "    print(f\"Tag: {part.name}\")\n",
        "    print(f\"Class: {part.get('class')}\")\n",
        "    print(f\"Content:\\n{part.get_text(strip=True)[:500]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fG_3GpHwQR7Z",
        "outputId": "5fa920fd-5cf3-4c1b-c333-ded01bc91dba"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXTRACTED PARTS:\n",
            "\n",
            "--- PART 1 ---\n",
            "Tag: header\n",
            "Class: ['post-header']\n",
            "Content:\n",
            "LLM Powered Autonomous AgentsDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "--- PART 2 ---\n",
            "Tag: h1\n",
            "Class: ['post-title']\n",
            "Content:\n",
            "LLM Powered Autonomous Agents\n",
            "\n",
            "--- PART 3 ---\n",
            "Tag: div\n",
            "Class: ['post-meta']\n",
            "Content:\n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "--- PART 4 ---\n",
            "Tag: div\n",
            "Class: ['post-content']\n",
            "Content:\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such asAutoGPT,GPT-EngineerandBabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.Agent System Overview#In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:PlanningSubgoal\n",
            "\n",
            "--- PART 5 ---\n",
            "Tag: p\n",
            "Class: None\n",
            "Content:\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such asAutoGPT,GPT-EngineerandBabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
            "\n",
            "--- PART 6 ---\n",
            "Tag: a\n",
            "Class: None\n",
            "Content:\n",
            "AutoGPT\n",
            "\n",
            "--- PART 7 ---\n",
            "Tag: a\n",
            "Class: None\n",
            "Content:\n",
            "GPT-Engineer\n",
            "\n",
            "--- PART 8 ---\n",
            "Tag: a\n",
            "Class: None\n",
            "Content:\n",
            "BabyAGI\n",
            "\n",
            "--- PART 9 ---\n",
            "Tag: h1\n",
            "Class: None\n",
            "Content:\n",
            "Agent System Overview#\n",
            "\n",
            "--- PART 10 ---\n",
            "Tag: a\n",
            "Class: ['anchor']\n",
            "Content:\n",
            "#\n",
            "\n",
            "--- PART 11 ---\n",
            "Tag: p\n",
            "Class: None\n",
            "Content:\n",
            "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
            "\n",
            "--- PART 12 ---\n",
            "Tag: ul\n",
            "Class: None\n",
            "Content:\n",
            "PlanningSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.MemoryShort-term memory: I would consider all the in-context learning (SeePrompt Engineering) as utilizing short-term memory of the model to learn.Long-\n",
            "\n",
            "--- PART 13 ---\n",
            "Tag: li\n",
            "Class: None\n",
            "Content:\n",
            "PlanningSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
            "\n",
            "--- PART 14 ---\n",
            "Tag: strong\n",
            "Class: None\n",
            "Content:\n",
            "Planning\n",
            "\n",
            "--- PART 15 ---\n",
            "Tag: ul\n",
            "Class: None\n",
            "Content:\n",
            "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s unpack this because it’s *super important* for how the loader → splitter → vector store flow works.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **What actually happens with multiple parts?**\n",
        "\n",
        "When you do this:\n",
        "\n",
        "```python\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs={\"parse_only\": bs4.SoupStrainer(\n",
        "        class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "    )}\n",
        ")\n",
        "docs = loader.load()\n",
        "```\n",
        "\n",
        "👉 **`loader.load()` returns a list of `Document` objects** — *one per matched chunk*.\n",
        "\n",
        "So if your page has:\n",
        "\n",
        "* `<h1 class=\"post-title\"> ... </h1>` → that’s **one Document**\n",
        "* `<header class=\"post-header\"> ... </header>` → another **Document**\n",
        "* `<div class=\"post-content\"> ... </div>` → another **Document**\n",
        "\n",
        "✅ So you may get `len(docs) == 3` for this blog.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 **Do they automatically combine?**\n",
        "\n",
        "**No!** `WebBaseLoader` does not automatically merge them.\n",
        "They stay separate in your `docs` list:\n",
        "\n",
        "```python\n",
        "[\n",
        "    Document(page_content=\"Agents: What are they?...\"),\n",
        "    Document(page_content=\"Author: Lilian Weng...\"),\n",
        "    Document(page_content=\"This post discusses ReAct, ...\")\n",
        "]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **When do they get combined?**\n",
        "\n",
        "They **don’t** get literally merged into one big `Document`.\n",
        "Instead, they go through your **TextSplitter**, which will:\n",
        "\n",
        "* Take each `Document` individually.\n",
        "* Chunk them into smaller overlapping parts.\n",
        "* The result is one long list of **smaller chunks**, all from all the original docs.\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 **Example:**\n",
        "\n",
        "```python\n",
        "# Suppose you have 3 docs:\n",
        "docs = loader.load()\n",
        "print(len(docs))  # 3\n",
        "\n",
        "# Then split:\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "print(len(splits))  # Maybe 20 or 30 chunks!\n",
        "```\n",
        "\n",
        "So in the end:\n",
        "✅ The chunks **represent the combined source**\n",
        "✅ But technically they come from separate source `Documents`\n",
        "✅ Each chunk keeps `metadata` → so your retrieval knows which source it came from!\n",
        "\n",
        "---\n",
        "\n",
        "## 🗂️ **Why this is good**\n",
        "\n",
        "This design means:\n",
        "\n",
        "* You can pull exactly the HTML bits you want → fine control.\n",
        "* You get clear metadata for each chunk.\n",
        "* Your retriever can later show you: *“This passage came from the post-content div, or the title header, etc.”*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MZlHHaX5RdQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(extracted_parts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hApH6u9kQR4s",
        "outputId": "5d5af324-b353-417d-fccc-abcd6b22bc73"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "401"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose you have 3 docs:\n",
        "docs = loader.load()\n",
        "print(len(docs))  # 3\n",
        "\n",
        "# Then split:\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "print(len(splits))  # Maybe 20 or 30 chunks!\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3wEkJoGQR18",
        "outputId": "2f07e7f9-2590-42d0-d152-a8f4dda7770e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_splits[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pxo2nrZ2WXs9",
        "outputId": "026cc959-eb9a-4533-8526-6a4471035cf5"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory')"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splits Inspection\n",
        "\n",
        "✅ **Fantastic insight — you’re asking exactly the right question!**\n",
        "You’re right: the *tidiness* of your final chunks **does directly affect** your RAG output quality. Let’s break down *why*, what’s happening, and how to clean it up if you want **higher-quality chunks**.\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 **What’s in your current split?**\n",
        "\n",
        "Your `all_splits[0]` is:\n",
        "\n",
        "```\n",
        "Document(\n",
        "  metadata={...},\n",
        "  page_content=(\n",
        "    \"LLM Powered Autonomous Agents\\n\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM...\"\n",
        "  )\n",
        ")\n",
        "```\n",
        "\n",
        "So your chunk has:\n",
        "\n",
        "* ✅ **Good content**: the intro, headings, paragraphs.\n",
        "* ⚠️ **Extra noise**: metadata like “Date: … | Author: …” that repeats in other chunks.\n",
        "* ⚠️ Extra `\\n` → doesn’t usually hurt the LLM, but lots of them can waste tokens or break flow.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Why does this matter for RAG?**\n",
        "\n",
        "* The embeddings you generate for similarity search are only as good as the text quality.\n",
        "* If every chunk carries repetitive noise (like the date/author for each blog post), it can:\n",
        "\n",
        "  * Dilute your semantic similarity.\n",
        "  * Waste tokens in your context window when you pass retrieved docs to the LLM.\n",
        "  * Make the final answer sound repetitive or cluttered.\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 **Where does this come from?**\n",
        "\n",
        "* This is from the **HTML structure**:\n",
        "\n",
        "  * For example, the “post-header” block probably includes the title, date, author.\n",
        "  * That’s perfectly fine *once* — but when you split the text, it can get copied into multiple chunks because the `RecursiveCharacterTextSplitter` doesn’t “know” what’s semantically important vs. repeated.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **How tidy is the final product by default?**\n",
        "\n",
        "👉 By default:\n",
        "\n",
        "* It’s **pretty good** — you’re getting meaningful, readable text blocks.\n",
        "* But you *can* get repeated headers or metadata you may not want.\n",
        "* The newline characters `\\n` are usually not a big deal — but for very clean retrieval, you might normalize them.\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡️ **How to clean it up (best practices)**\n",
        "\n",
        "### ✅ 1️⃣ Strip or filter unwanted text before splitting\n",
        "\n",
        "**One-time removal is the best place:**\n",
        "\n",
        "```python\n",
        "# Inspect your docs before splitting:\n",
        "for doc in docs:\n",
        "    print(\"--- RAW DOC ---\")\n",
        "    print(doc.page_content[:500])\n",
        "\n",
        "# Example cleanup:\n",
        "for doc in docs:\n",
        "    text = doc.page_content\n",
        "\n",
        "    # Example: Remove the Date/Author line if it's always the same\n",
        "    text = text.replace(\"Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\", \"\")\n",
        "\n",
        "    # Collapse multiple newlines\n",
        "    text = \"\\n\".join([line.strip() for line in text.splitlines() if line.strip()])\n",
        "\n",
        "    doc.page_content = text\n",
        "```\n",
        "\n",
        "Then split:\n",
        "\n",
        "```python\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "```\n",
        "\n",
        "✅ This means your **chunks will never contain that redundant metadata**, so your embeddings will be clearer.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 2️⃣ Or use a more advanced cleaner\n",
        "\n",
        "You could also:\n",
        "\n",
        "* Use regex to strip repeated footers, headers, disclaimers, timestamps.\n",
        "* Apply `strip()` to remove leading/trailing whitespace.\n",
        "* Remove line breaks if you don’t need them for semantic structure.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 3️⃣ Normalizing whitespace helps\n",
        "\n",
        "LLMs handle `\\n` fine — they even use them in chain-of-thought. But random extra line breaks can waste tokens.\n",
        "One common trick:\n",
        "\n",
        "```python\n",
        "def clean_text(text):\n",
        "    # Remove extra newlines\n",
        "    lines = text.splitlines()\n",
        "    lines = [line.strip() for line in lines if line.strip()]\n",
        "    return \" \".join(lines)\n",
        "\n",
        "for doc in docs:\n",
        "    doc.page_content = clean_text(doc.page_content)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 **Does this really improve RAG performance?**\n",
        "\n",
        "✅ YES — for real-world projects, *cleaner* chunks:\n",
        "\n",
        "* Improve embedding similarity.\n",
        "* Reduce hallucination risk because retrieval is more on-point.\n",
        "* Use your token budget more effectively → less wasted cost.\n",
        "\n",
        "---\n",
        "\n",
        "## 🟢 **So your instinct is 100% right**\n",
        "\n",
        "| Noisy chunks               | Cleaned chunks                    |\n",
        "| -------------------------- | --------------------------------- |\n",
        "| Date/author in every chunk | Appears once, or only in metadata |\n",
        "| Extra whitespace           | Normalized                        |\n",
        "| Poor semantic flow         | Improved relevance                |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ ✅ ✅ **Quick takeaway**\n",
        "\n",
        "* The default tutorial gives you a *decent* baseline.\n",
        "* But a tiny bit of **text pre-processing** can noticeably improve your results — especially for larger RAG projects.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o7G1VvDvTEY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, split in enumerate(all_splits[:10]):  # First 10 splits\n",
        "    print(f\"--- Chunk {i+1} ---\")\n",
        "    print(f\"Length (chars): {len(split.page_content)}\")\n",
        "    print(f\"Metadata: {split.metadata}\")\n",
        "    print(\"\\nCONTENT:\\n\")\n",
        "    print(split.page_content[:500])  # Preview first 500 chars\n",
        "    print('\\n' + '-'*80 + '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSotsy9qUo7I",
        "outputId": "b04de122-b1fa-4850-8eaf-50d43a6c38bc"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Chunk 1 ---\n",
            "Length (chars): 969\n",
            "Metadata: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
            "\n",
            "CONTENT:\n",
            "\n",
            "LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
            "Agent System Overview#\n",
            "In a LLM-p\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Chunk 2 ---\n",
            "Length (chars): 665\n",
            "Metadata: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
            "\n",
            "CONTENT:\n",
            "\n",
            "Memory\n",
            "\n",
            "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
            "Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n",
            "\n",
            "\n",
            "Tool use\n",
            "\n",
            "The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Chunk 3 ---\n",
            "Length (chars): 939\n",
            "Metadata: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
            "\n",
            "CONTENT:\n",
            "\n",
            "Component One: Planning#\n",
            "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
            "Task Decomposition#\n",
            "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Chunk 4 ---\n",
            "Length (chars): 987\n",
            "Metadata: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
            "\n",
            "CONTENT:\n",
            "\n",
            "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
            "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Chunk 5 ---\n",
            "Length (chars): 760\n",
            "Metadata: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
            "\n",
            "CONTENT:\n",
            "\n",
            "Self-Reflection#\n",
            "Self-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\n",
            "ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikip\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Chunk 6 ---\n",
            "Length (chars): 974\n",
            "Metadata: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
            "\n",
            "CONTENT:\n",
            "\n",
            "Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\n",
            "\n",
            "In both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\n",
            "Reflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a stand\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Chunk 7 ---\n",
            "Length (chars): 858\n",
            "Metadata: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
            "\n",
            "CONTENT:\n",
            "\n",
            "Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\n",
            "\n",
            "The heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\n",
            "Self-reflection is created by showing two-shot examples to LLM and each example is a\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Chunk 8 ---\n",
            "Length (chars): 960\n",
            "Metadata: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
            "\n",
            "CONTENT:\n",
            "\n",
            "Chain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\geq r_{n-1} \\geq \\d\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Chunk 9 ---\n",
            "Length (chars): 412\n",
            "Metadata: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
            "\n",
            "CONTENT:\n",
            "\n",
            "To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\n",
            "The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Chunk 10 ---\n",
            "Length (chars): 989\n",
            "Metadata: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
            "\n",
            "CONTENT:\n",
            "\n",
            "After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\n",
            "\n",
            "The idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-con\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Split the text into lines\n",
        "    lines = text.splitlines()\n",
        "\n",
        "    # Strip each line of leading/trailing whitespace and drop empty lines\n",
        "    lines = [line.strip() for line in lines if line.strip()]\n",
        "\n",
        "    # Join the cleaned lines into one string with single spaces\n",
        "    return \" \".join(lines)\n",
        "\n",
        "\n",
        "# Pick a doc to inspect\n",
        "original = docs[0].page_content\n",
        "\n",
        "# Clean it\n",
        "cleaned = clean_text(original)\n",
        "\n",
        "# Compare\n",
        "import textwrap\n",
        "from pprint import pprint  # For pretty-printing dicts if needed\n",
        "\n",
        "# Compare side by side with nice wrapping\n",
        "print(\"--- ORIGINAL ---\")\n",
        "print(\"\\n\".join(textwrap.wrap(original[:500], width=80)))\n",
        "\n",
        "print(\"\\n--- CLEANED ---\")\n",
        "print(\"\\n\".join(textwrap.wrap(cleaned[:500], width=80)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tA9cpLEtT8IR",
        "outputId": "676ffbe2-35b6-4046-f284-8cfd48d601dd"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ORIGINAL ---\n",
            "LLM Powered Autonomous Agents Building agents with LLM (large language model) as\n",
            "its core controller is a cool concept. Several proof-of-concepts demos, such as\n",
            "AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality\n",
            "of LLM extends beyond generating well-written copies, stories, essays and\n",
            "programs; it can be framed as a powerful general problem solver. Agent System\n",
            "Overview# In a LLM-powered autonomous agent system, LLM functions as the agent’s\n",
            "brain, complemented by se\n",
            "\n",
            "--- CLEANED ---\n",
            "LLM Powered Autonomous Agents Building agents with LLM (large language model) as\n",
            "its core controller is a cool concept. Several proof-of-concepts demos, such as\n",
            "AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality\n",
            "of LLM extends beyond generating well-written copies, stories, essays and\n",
            "programs; it can be framed as a powerful general problem solver. Agent System\n",
            "Overview# In a LLM-powered autonomous agent system, LLM functions as the agent’s\n",
            "brain, complemented by se\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt\n",
        "\n",
        "## 📌 **What is `hub.pull(\"rlm/rag-prompt\")` doing?**\n",
        "\n",
        "In the new LangChain 0.2+ framework, **`hub.pull`** loads a prompt **from the LangChain Hub**, which is a shared place to store reusable prompts, chains, and components.\n",
        "\n",
        "```python\n",
        "from langchain import hub\n",
        "\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "```\n",
        "\n",
        "So `rlm/rag-prompt` is:\n",
        "\n",
        "* `rlm` → the username or org on the Hub.\n",
        "* `rag-prompt` → the name of the prompt they published.\n",
        "\n",
        "This lets you reuse **battle-tested** prompt templates instead of writing one from scratch.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Can you see the prompt online?**\n",
        "\n",
        "✔️ Yes! Every public Hub asset has a **web page**.\n",
        "👉 The URL pattern is:\n",
        "\n",
        "```\n",
        "https://smith.langchain.com/hub/<username>/<asset-name>\n",
        "```\n",
        "\n",
        "So in your case:\n",
        "\n",
        "```\n",
        "https://smith.langchain.com/hub/rlm/rag-prompt\n",
        "```\n",
        "\n",
        "If you open that, you’ll see:\n",
        "\n",
        "* The full prompt template.\n",
        "* Inputs it expects (e.g., `{question}` and `{context}`).\n",
        "* Example usage.\n",
        "* Versions, if any.\n",
        "\n",
        "---\n",
        "\n",
        "## 🟢 **What does this prompt contain?**\n",
        "\n",
        "Here’s what a standard **RAG prompt** usually looks like:\n",
        "\n",
        "```\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "If you don’t know the answer, just say you don’t know — don’t try to make up an answer.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Helpful Answer:\n",
        "```\n",
        "\n",
        "✅ It ensures:\n",
        "\n",
        "* The LLM grounds its answer in your retrieved chunks.\n",
        "* It avoids hallucinations by telling the LLM not to guess.\n",
        "* Your `{context}` and `{question}` are plugged in dynamically when you call `prompt.invoke(...)`.\n",
        "\n",
        "---\n",
        "\n",
        "## 🗂️ **How can you inspect it locally?**\n",
        "\n",
        "After pulling it:\n",
        "\n",
        "```python\n",
        "print(prompt)\n",
        "```\n",
        "\n",
        "or:\n",
        "\n",
        "```python\n",
        "print(prompt.messages)\n",
        "```\n",
        "\n",
        "Depending on whether it’s a `PromptTemplate`, `ChatPromptTemplate`, or some other type, you’ll see its structure.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Key takeaway**\n",
        "\n",
        "| Thing                                            | What it does                                                 |\n",
        "| ------------------------------------------------ | ------------------------------------------------------------ |\n",
        "| `hub.pull(\"rlm/rag-prompt\")`                     | Loads a reusable prompt template from the Hub                |\n",
        "| `https://smith.langchain.com/hub/rlm/rag-prompt` | Lets you view the exact prompt                               |\n",
        "| `prompt.invoke({...})`                           | Fills in `{question}` and `{context}` when you run the graph |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ ✅ ✅ **Next**\n",
        "\n",
        "* Go check it out at [smith.langchain.com/hub/rlm/rag-prompt](https://smith.langchain.com/hub/rlm/rag-prompt)\n"
      ],
      "metadata": {
        "id": "kPrUedaieI-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The prompt is a ChatPromptTemplate with one or more messages\n",
        "print(type(prompt))\n",
        "# <class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
        "\n",
        "# The actual list of messages\n",
        "print(prompt.messages)\n",
        "# [HumanMessagePromptTemplate(...), ...]\n",
        "\n",
        "# Grab the first HumanMessagePromptTemplate\n",
        "human_message = prompt.messages[0]\n",
        "\n",
        "print(type(human_message))\n",
        "# <class 'langchain_core.prompts.chat.HumanMessagePromptTemplate'>\n",
        "\n",
        "# That has a .prompt which is your underlying PromptTemplate\n",
        "underlying_template = human_message.prompt\n",
        "\n",
        "print(type(underlying_template))\n",
        "# <class 'langchain_core.prompts.prompt.PromptTemplate'>\n",
        "\n",
        "# Finally, get the raw template string!\n",
        "print('\\n')\n",
        "print(\"\\n\".join(textwrap.wrap(underlying_template.template, width=80)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dpip2TeKT8Fa",
        "outputId": "6e64244b-cf32-454d-8a16-6bdb326269bb"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
            "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n",
            "<class 'langchain_core.prompts.chat.HumanMessagePromptTemplate'>\n",
            "<class 'langchain_core.prompts.prompt.PromptTemplate'>\n",
            "\n",
            "\n",
            "You are an assistant for question-answering tasks. Use the following pieces of\n",
            "retrieved context to answer the question. If you don't know the answer, just say\n",
            "that you don't know. Use three sentences maximum and keep the answer concise.\n",
            "Question: {question}  Context: {context}  Answer:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ **Good RAG prompts are often surprisingly simple.**\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 **Why is it so simple?**\n",
        "\n",
        "Your prompt:\n",
        "\n",
        "```\n",
        "You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use three sentences maximum and keep the answer concise.\n",
        "\n",
        "Question: {question}  \n",
        "Context: {context}  \n",
        "Answer:\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🔑 **What this does right**\n",
        "\n",
        "1️⃣ **Grounds the LLM**\n",
        "\n",
        "* It says: *“Use the context I gave you. Don’t hallucinate.”*\n",
        "\n",
        "2️⃣ **Has a clear fallback**\n",
        "\n",
        "* *“If you don’t know, just say you don’t know.”*\n",
        "* This reduces confident nonsense when the context is irrelevant or missing.\n",
        "\n",
        "3️⃣ **Keeps output short**\n",
        "\n",
        "* *“Three sentences max, keep it concise.”*\n",
        "* Saves tokens and keeps answers direct — especially important if you show the source passages to the user too.\n",
        "\n",
        "4️⃣ **No unnecessary instructions**\n",
        "\n",
        "* No fancy formatting, references, or style — just plain QA.\n",
        "* The retriever does the heavy lifting: the better your chunks, the better this works.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Good RAG = simple prompt + good chunks**\n",
        "\n",
        "You nailed it:\n",
        "\n",
        "* The *real power* comes from **high-quality, well-chunked, relevant context**.\n",
        "* A fancy prompt can’t fix irrelevant or noisy chunks.\n",
        "* A simple, direct prompt works better because the LLM doesn’t waste tokens or logic on unnecessary instructions.\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 **When you might expand it**\n",
        "\n",
        "You’d only make your RAG prompt more complex when you need:\n",
        "\n",
        "* ✅ Citations: *“Add \\[1], \\[2], \\[3] next to facts.”*\n",
        "* ✅ Structured output: JSON, bullet lists, or tables.\n",
        "* ✅ A custom style or tone: e.g., summarizing in plain language for kids.\n",
        "* ✅ Additional constraints: like always adding an explicit source URL.\n",
        "\n",
        "But for baseline QA → **short, direct instructions are king**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🟢 **Key takeaway**\n",
        "\n",
        "| Good chunk quality          | Good retriever                                   | Simple grounding prompt                                |\n",
        "| --------------------------- | ------------------------------------------------ | ------------------------------------------------------ |\n",
        "| ✅ Clean text, minimal noise | ✅ Similarity search that finds relevant passages | ✅ “Use this context. Be concise. Don’t make stuff up.” |\n",
        "\n",
        "This is what gives you reliable, traceable answers.\n",
        "\n"
      ],
      "metadata": {
        "id": "yCQoZIJEf-CU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> *“How important is prepping and optimizing my source documentation for RAG?”*\n",
        "\n",
        "The short answer: **It’s absolutely critical — maybe the single most important thing you control!**\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 **Why your chunk quality matters more than a fancy prompt**\n",
        "\n",
        "### 🗂️ **RAG is really just two big steps:**\n",
        "\n",
        "1️⃣ **Retrieve** — Use embeddings + vector store to find relevant text chunks.\n",
        "2️⃣ **Generate** — Ask the LLM to answer your question using those chunks.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚡ **What does “garbage in, garbage out” mean for RAG?**\n",
        "\n",
        "👉 If your source text is noisy, irrelevant, repetitive, or poorly chunked:\n",
        "\n",
        "* The vector store will return off-topic or low-quality context.\n",
        "* The LLM will either hallucinate to fill gaps **OR** parrot irrelevant details.\n",
        "* No prompt in the world can fix “bad chunks in, bad answer out.”\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **The retrieval step is your “truth filter.”**\n",
        "\n",
        "A clear, tight prompt just says:\n",
        "\n",
        "> “Hey model, trust these chunks. Don’t guess.”\n",
        "\n",
        "But if the chunks themselves are:\n",
        "\n",
        "* Missing key facts\n",
        "* Full of boilerplate (headers, footers, legal disclaimers)\n",
        "* Or too big and vague\n",
        "\n",
        "…the LLM has no good raw material to pull an accurate answer from.\n",
        "\n",
        "---\n",
        "\n",
        "## 🟢 **What makes a chunk “high quality”?**\n",
        "\n",
        "✅ Clear, well-written text — free from extra noise like navbars, unrelated disclaimers.\n",
        "\n",
        "✅ Right chunk size — small enough to stay relevant (\\~500–1000 tokens), large enough to preserve meaning.\n",
        "\n",
        "✅ Overlap for context — to prevent splitting related ideas mid-paragraph.\n",
        "\n",
        "✅ Consistent format — so your embeddings stay semantically sharp.\n",
        "\n",
        "✅ Enriched metadata — so you can filter and display source details later.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧹 **How do people prep their docs well?**\n",
        "\n",
        "📄 **Good examples:**\n",
        "\n",
        "* Strip repeated boilerplate: e.g., headers/footers that repeat every page.\n",
        "* Remove unrelated sections: e.g., site nav, social share buttons.\n",
        "* Normalize whitespace: e.g., clean up newlines, markdown, code blocks.\n",
        "* Keep logical units together: e.g., don’t break a section heading from its paragraph.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **How big is the impact?**\n",
        "\n",
        "There’s tons of evidence (and real-world LangChain teams see this daily!):\n",
        "\n",
        "* Improving chunk quality can improve RAG accuracy dramatically — often more than tweaking your LLM or prompt.\n",
        "* If your chunks are noisy, similarity search pulls bad matches.\n",
        "* Better chunks = better semantic similarity = more relevant context = less hallucination.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔑 **Your secret RAG formula**\n",
        "\n",
        "| Part                          | How much control you have         | Impact      |\n",
        "| ----------------------------- | --------------------------------- | ----------- |\n",
        "| Chunk quality & text cleaning | ✅ 100%                            | 🔥 Massive  |\n",
        "| Chunk size & overlap          | ✅ 100%                            | 🔥 High     |\n",
        "| Prompt clarity                | ✅ 100%                            | 🔥 Medium   |\n",
        "| LLM model choice              | ✅ 50% (API pricing & performance) | 🔥 Moderate |\n",
        "| Vector store tech             | ✅ 100%                            | 🔥 Medium   |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ ✅ ✅ **Key takeaway**\n",
        "\n",
        "> 🏆 *“RAG is mostly about smart retrieval.\n",
        "> Smart retrieval is all about high-quality, clean chunks.”*\n",
        "\n",
        "A fancy prompt is just the icing on the cake.\n",
        "A good retriever *is the cake.* 🍰✨\n"
      ],
      "metadata": {
        "id": "LW6nN0CUiJJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt['template']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "bEHXoXSiYo08",
        "outputId": "0bfde1b7-8470-4d4c-d765-d540f52b8e95"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "list indices must be integers or slices, not str",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-59-2082241890.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprompt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'template'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/prompts/chat.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m   1288\u001b[0m             \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mChatPromptTemplate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgVigGvPe0cO",
        "outputId": "b3692446-e2b2-48d0-f925-7b40e0f568b5"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an assistant for question-answering tasks. Use the following pieces of\n",
            "retrieved context to answer the question. If you don't know the answer, just say\n",
            "that you don't know. Use three sentences maximum and keep the answer concise.\n",
            "Question: {question}  Context: {context}  Answer:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DDePHLUXfONm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}