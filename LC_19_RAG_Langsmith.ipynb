{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNvTODdXJghWXqIVTV+gPul",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/RAG-LangChain/blob/main/LC_19_RAG_Langsmith.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ✅ **`%pip` vs. `%%pip` vs. `!pip`**\n",
        "\n",
        "1️⃣ **`!pip install ...`**\n",
        "\n",
        "* This runs `pip` in a **shell subprocess** (like typing in a terminal).\n",
        "* In Colab/Jupyter, this can sometimes install packages into a different Python environment than the one the notebook kernel uses — especially if multiple Python versions or virtual environments are involved.\n",
        "* You might install a package but then `import` still fails → annoying!\n",
        "\n",
        "2️⃣ **`%pip install ...`**\n",
        "\n",
        "* `%pip` is an **IPython magic command** (single-line).\n",
        "* It ensures that the package is installed in **the same Python environment as your notebook kernel**.\n",
        "* It’s the recommended way for pip installs in Jupyter/Colab.\n",
        "\n",
        "✅ Use `%pip` instead of `!pip` to avoid “module not found” surprises.\n",
        "\n",
        "3️⃣ **`%%pip install ...`**\n",
        "\n",
        "* `%%pip` is the **cell magic** version of `%pip`.\n",
        "* It works exactly the same way but applies to the **entire cell**.\n",
        "* Useful if you have multiple install lines in one cell.\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡ **Example**\n",
        "\n",
        "```python\n",
        "# Good practice in Google Colab\n",
        "%pip install langchain-openai langsmith\n",
        "\n",
        "# Or, using cell magic:\n",
        "%%pip install langchain-openai langsmith\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🗂️ **What about `%%capture`?**\n",
        "\n",
        "You wrote:\n",
        "\n",
        "```python\n",
        "%%capture --no-stderr\n",
        "%pip install ...\n",
        "```\n",
        "\n",
        "* `%%capture` is another IPython cell magic that **captures stdout and stderr** (so you don’t see all the pip output).\n",
        "* `--no-stderr` means it won’t capture errors → you’ll still see them if pip fails.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Best practice for Colab**\n",
        "\n",
        "* Use `%pip` or `%%pip` instead of `!pip` for Python packages.\n",
        "* Use `%%capture` if you want to hide noisy output.\n",
        "* Always restart your runtime (`Runtime → Restart runtime`) after major installs, if needed.\n",
        "\n"
      ],
      "metadata": {
        "id": "2db_kAIwGQdv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VaCkH33pECeN"
      },
      "outputs": [],
      "source": [
        "# %%capture --no-stderr\n",
        "# %pip install langsmith langchain-openai langchain-core langchain-community pydantic python-dotenv openai langgraph\n",
        "# %pip install --upgrade langsmith"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -Uq langsmith langchain-openai langchain-core langchain-community pydantic python-dotenv openai langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v47INGDfGWDq",
        "outputId": "df5bca19-0638-4fe8-d415-7847af55bb54"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.4/441.4 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.1/755.1 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.7/143.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environ Setup"
      ],
      "metadata": {
        "id": "ET_-WfA5Ja74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1) Imports ---\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# --- 2) Load environment variables ---\n",
        "load_dotenv(\"/content/API_KEYS.env\", override=True)\n",
        "\n",
        "# Confirm keys\n",
        "print(\"OPENAI_API_KEY:\", os.getenv(\"OPENAI_API_KEY\"))\n",
        "print(\"LANGCHAIN_API_KEY:\", os.getenv(\"LANGCHAIN_API_KEY\"))\n",
        "print(\"LANGCHAIN_PROJECT:\", os.getenv(\"LANGCHAIN_PROJECT\"))\n",
        "print(\"LANGCHAIN_TRACING_V2:\", os.getenv(\"LANGCHAIN_TRACING_V2\"))\n",
        "\n",
        "# Alternatively set project name for this run outside .env file\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"project_00\"\n",
        "os.environ[\"USER_AGENT\"] = \"MyRAGApp/0.1\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMRJP96sEVGe",
        "outputId": "84612ad4-294d-495c-da14-a5e76f36f3a9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OPENAI_API_KEY: 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\n",
            "LANGCHAIN_API_KEY: None\n",
            "LANGCHAIN_PROJECT: my_project_name\n",
            "LANGCHAIN_TRACING_V2: true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Selection"
      ],
      "metadata": {
        "id": "E9GZhgEEJYoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
      ],
      "metadata": {
        "id": "kuQ_Y4HtJXH2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings\n",
        "\n",
        "✅ Perfect — you’re doing it exactly right for embeddings!\n",
        "Let me clarify how this works so you know you’re good to go:\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Using `OpenAIEmbeddings`**\n",
        "\n",
        "```python\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "```\n",
        "\n",
        "**What’s happening?**\n",
        "\n",
        "* `OpenAIEmbeddings` is the LangChain wrapper that calls OpenAI’s embedding endpoint.\n",
        "* `model=\"text-embedding-3-large\"` is the recommended newer embedding model (high quality).\n",
        "* Your `OPENAI_API_KEY` must be set (which you have in your `.env`).\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 **How it works**\n",
        "\n",
        "When you call:\n",
        "\n",
        "```python\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "```\n",
        "\n",
        "or:\n",
        "\n",
        "```python\n",
        "embeddings.embed_query(\"My question?\")\n",
        "```\n",
        "\n",
        "LangChain calls OpenAI’s embedding endpoint, gets the vector, and returns it to your retriever or vector store.\n"
      ],
      "metadata": {
        "id": "k3HwvlY0KLIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "\n",
        "# ⚡️ Tip: inspect embedding\n",
        "vector = embeddings.embed_query(\"How does LangSmith tracing work?\")\n",
        "print(len(vector))  # Should match the embedding dimension"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgB4R-KNKIUX",
        "outputId": "7ea85ec6-a659-4863-dbd0-c915d865538c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector Store"
      ],
      "metadata": {
        "id": "hiVLCWhKJ9iX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "vector_store = InMemoryVectorStore(embeddings)"
      ],
      "metadata": {
        "id": "bI2jUqIXJ8K-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import List, TypedDict\n",
        "\n",
        "# Load and chunk contents of the blog\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# Index chunks\n",
        "_ = vector_store.add_documents(documents=all_splits)\n",
        "\n",
        "# Define prompt for question-answering\n",
        "# N.B. for non-US LangSmith endpoints, you may need to specify\n",
        "# api_url=\"https://api.smith.langchain.com\" in hub.pull.\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "\n",
        "# Define state for application\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    answer: str\n",
        "\n",
        "\n",
        "# Define application steps\n",
        "def retrieve(state: State):\n",
        "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "\n",
        "def generate(state: State):\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"answer\": response.content}\n",
        "\n",
        "\n",
        "# Compile application and test\n",
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "YLD5DiFLLcli"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Breakdown\n",
        "\n",
        "Let’s break this block down step-by-step so you really see **what’s happening and why** — because this is a **perfect small RAG + LangGraph example** and there’s a lot to learn here! 🚀\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **🔹 1) Imports**\n",
        "\n",
        "```python\n",
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import List, TypedDict\n",
        "```\n",
        "\n",
        "* `bs4` → BeautifulSoup: you’ll use it to parse the HTML blog post.\n",
        "* `hub` → LangChain Hub: stores reusable prompts/templates.\n",
        "* `WebBaseLoader` → loads text from a webpage.\n",
        "* `Document` → LangChain’s document wrapper (metadata, page content).\n",
        "* `RecursiveCharacterTextSplitter` → splits long docs into chunks.\n",
        "* `StateGraph` → the new LCEL-based graph execution framework.\n",
        "* `TypedDict` → defines your app state type for type safety.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **🔹 2) Load and parse blog post**\n",
        "\n",
        "```python\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "```\n",
        "\n",
        "👉 This:\n",
        "\n",
        "* Loads **Lilian Weng’s “Agent” blog post**.\n",
        "* Uses `bs4.SoupStrainer` to keep only useful HTML parts (post content, title, header) → more focused chunks.\n",
        "* Wraps the text as `Document` objects.\n",
        "\n",
        "✅ **Why?** You don’t want your embeddings polluted by navbars, footers, or ads!\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **🔹 3) Split into chunks**\n",
        "\n",
        "```python\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "```\n",
        "\n",
        "👉 This:\n",
        "\n",
        "* Breaks long text into smaller, overlapping chunks (RAG best practice).\n",
        "* `chunk_size=1000`: each chunk \\~1000 characters.\n",
        "* `chunk_overlap=200`: 200 chars overlap between chunks to preserve context.\n",
        "\n",
        "✅ **Why?** Smaller chunks = better similarity search and fewer tokens sent to LLM.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **🔹 4) Index chunks**\n",
        "\n",
        "```python\n",
        "_ = vector_store.add_documents(documents=all_splits)\n",
        "```\n",
        "\n",
        "👉 This:\n",
        "\n",
        "* Adds the chunks to your vector store (`InMemoryVectorStore` or `FAISS`).\n",
        "* Each chunk is embedded and stored for similarity search later.\n",
        "\n",
        "✅ **Why?** This builds your local knowledge base to support retrieval.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **🔹 5) Pull RAG prompt from LangChain Hub**\n",
        "\n",
        "```python\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "```\n",
        "\n",
        "👉 This:\n",
        "\n",
        "* Loads a reusable RAG-style prompt from the LangChain Hub.\n",
        "* The prompt will look like:\n",
        "\n",
        "  ```\n",
        "  Use the following context to answer the question.\n",
        "  Question: {question}\n",
        "  Context: {context}\n",
        "  Answer:\n",
        "  ```\n",
        "\n",
        "✅ **Why?** Standardizes your RAG behavior without hardcoding your own prompt.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **🔹 6) Define app state**\n",
        "\n",
        "```python\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    answer: str\n",
        "```\n",
        "\n",
        "👉 This:\n",
        "\n",
        "* Defines the **input, intermediate, and output state** for your graph.\n",
        "* `question` → the user’s query.\n",
        "* `context` → retrieved docs.\n",
        "* `answer` → final LLM output.\n",
        "\n",
        "✅ **Why?** `StateGraph` needs a clear state schema to pass data between steps.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **🔹 7) Define steps**\n",
        "\n",
        "### `retrieve`\n",
        "\n",
        "```python\n",
        "def retrieve(state: State):\n",
        "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
        "    return {\"context\": retrieved_docs}\n",
        "```\n",
        "\n",
        "👉 This:\n",
        "\n",
        "* Takes the question.\n",
        "* Uses similarity search to find relevant chunks.\n",
        "* Returns them as `context` for the next step.\n",
        "\n",
        "✅ **Why?** Classic RAG step: “given a question, find matching chunks.”\n",
        "\n",
        "---\n",
        "\n",
        "### `generate`\n",
        "\n",
        "```python\n",
        "def generate(state: State):\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"answer\": response.content}\n",
        "```\n",
        "\n",
        "👉 This:\n",
        "\n",
        "* Joins retrieved chunks into a single context block.\n",
        "* Calls the prompt to format the input for the LLM.\n",
        "* Calls the LLM to generate the answer.\n",
        "* Returns the answer as your final state.\n",
        "\n",
        "✅ **Why?** This is the generation half of RAG: “use the retrieved context to produce a grounded answer.”\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **🔹 8) Build & compile your `StateGraph`**\n",
        "\n",
        "```python\n",
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "graph = graph_builder.compile()\n",
        "```\n",
        "\n",
        "👉 This:\n",
        "\n",
        "* Creates a graph where:\n",
        "\n",
        "  * **Start** → `retrieve` → `generate`\n",
        "* `StateGraph` wires up your pipeline with a clear flow.\n",
        "\n",
        "✅ **Why?** This modular graph pattern is reusable, composable, and debuggable — much better than messy nested calls!\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **🔹 9) Next step: Run it**\n",
        "\n",
        "After this block, you run:\n",
        "\n",
        "```python\n",
        "result = graph.invoke({\"question\": \"What are ReAct agents?\"})\n",
        "print(result)\n",
        "```\n",
        "\n",
        "* This runs `retrieve` → `generate` → gives you an answer.\n",
        "* ✅ And because your env vars are set, the entire run logs to **LangSmith**:\n",
        "\n",
        "  * Question\n",
        "  * Retrieved docs\n",
        "  * LLM output\n",
        "  * Token usage\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **📌 What this block gives you**\n",
        "\n",
        "✔️ A **clean RAG pipeline** with:\n",
        "\n",
        "* **Data loading**\n",
        "* **Chunking**\n",
        "* **Embeddings + vector store**\n",
        "* **Retrieval**\n",
        "* **Prompt templating**\n",
        "* **LLM generation**\n",
        "* **Modular graph orchestration**\n",
        "\n",
        "✔️ **Full observability** via LangSmith (if your API key & project are correct).\n",
        "\n",
        "---\n",
        "\n",
        "## 🎉 **Why this is powerful**\n",
        "\n",
        "* You can swap the retriever, LLM, or prompt independently.\n",
        "* You can add more steps — like evals, re-ranking, or feedback loops.\n",
        "* You can see the entire flow in your LangSmith dashboard to debug or tune.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-JSPckvzOA9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Soup Strainer"
      ],
      "metadata": {
        "id": "YIcN5L7uOrOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect raw HTML (not using SoupStrainer)\n",
        "import requests\n",
        "html = requests.get(\"https://lilianweng.github.io/posts/2023-06-23-agent/\").text\n",
        "\n",
        "print(html[:1000])  # Show first 1000 chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MbsZMS_OzML",
        "outputId": "0a96d616-28a2-43ef-d9e7-831a829b4848"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!DOCTYPE html>\n",
            "<html lang=\"en\" dir=\"auto\">\n",
            "\n",
            "<head><meta charset=\"utf-8\">\n",
            "<meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n",
            "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\">\n",
            "<meta name=\"robots\" content=\"index, follow\">\n",
            "<title>LLM Powered Autonomous Agents | Lil&#39;Log</title>\n",
            "<meta name=\"keywords\" content=\"nlp, language-model, agent, steerability, prompting\" />\n",
            "<meta name=\"description\" content=\"Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
            "Agent System Overview\n",
            "In a LLM-powered autonomous agent system, LLM functions as the agent&rsquo;s brain, complemented by several key components:\n",
            "\n",
            "Planning\n",
            "\n",
            "Subgoal and decomposition: The agent breaks down lar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(docs))\n",
        "print(docs[0].page_content[:500])  # show a snippet\n",
        "print(docs[0].metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zZkvdUUJW_l",
        "outputId": "419ff83c-7de2-43b4-c274-60e94a9dcc8d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "\n",
            "\n",
            "      LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
            "Agent System Overview#\n",
            "In\n",
            "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup, SoupStrainer\n",
        "\n",
        "# Define what you want to extract:\n",
        "only_useful = SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))\n",
        "\n",
        "# Parse HTML, filtering at parse time:\n",
        "soup = BeautifulSoup(html, \"html.parser\", parse_only=only_useful)\n",
        "\n",
        "# Check what you got:\n",
        "print(soup.prettify()[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVNBYuMsJXB_",
        "outputId": "c0687528-725a-417d-ed56-445f1e686fbd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<header class=\"post-header\">\n",
            " <h1 class=\"post-title\">\n",
            "  LLM Powered Autonomous Agents\n",
            " </h1>\n",
            " <div class=\"post-meta\">\n",
            "  Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            " </div>\n",
            "</header>\n",
            "<div class=\"post-content\">\n",
            " <p>\n",
            "  Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as\n",
            "  <a href=\"https://github.com/Significant-Gravitas/Auto-GPT\">\n",
            "   AutoGPT\n",
            "  </a>\n",
            "  ,\n",
            "  <a href=\"https://github.com/AntonOsika/gpt-engineer\">\n",
            "   GPT-Engineer\n",
            "  </a>\n",
            "  and\n",
            "  <a href=\"https://github.com/yoheinakajima/babyagi\">\n",
            "   BabyAGI\n",
            "  </a>\n",
            "  , serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
            " </p>\n",
            " <h1 id=\"agent-system-overview\">\n",
            "  Agent System Overview\n",
            "  <a aria-hidden=\"true\" class=\"anchor\" hidden=\"\" href=\"#agent-system-overview\">\n",
            "   #\n",
            "  </a>\n",
            " </h1>\n",
            " <p>\n",
            "  In a LLM-powered au\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup, SoupStrainer\n",
        "\n",
        "url = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
        "\n",
        "# Step 1: Get HTML\n",
        "html = requests.get(url).text\n",
        "\n",
        "# Step 2: Strain for useful parts\n",
        "only_useful = SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))\n",
        "soup = BeautifulSoup(html, \"html.parser\", parse_only=only_useful)\n",
        "\n",
        "# Step 3: Inspect\n",
        "print(\"EXTRACTED PARTS:\")\n",
        "extracted_parts = soup.find_all(True)\n",
        "\n",
        "for idx, part in enumerate(extracted_parts[0:15]):\n",
        "    print(f\"\\n--- PART {idx+1} ---\")\n",
        "    print(f\"Tag: {part.name}\")\n",
        "    print(f\"Class: {part.get('class')}\")\n",
        "    print(f\"Content:\\n{part.get_text(strip=True)[:500]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fG_3GpHwQR7Z",
        "outputId": "5fa920fd-5cf3-4c1b-c333-ded01bc91dba"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXTRACTED PARTS:\n",
            "\n",
            "--- PART 1 ---\n",
            "Tag: header\n",
            "Class: ['post-header']\n",
            "Content:\n",
            "LLM Powered Autonomous AgentsDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "--- PART 2 ---\n",
            "Tag: h1\n",
            "Class: ['post-title']\n",
            "Content:\n",
            "LLM Powered Autonomous Agents\n",
            "\n",
            "--- PART 3 ---\n",
            "Tag: div\n",
            "Class: ['post-meta']\n",
            "Content:\n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "--- PART 4 ---\n",
            "Tag: div\n",
            "Class: ['post-content']\n",
            "Content:\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such asAutoGPT,GPT-EngineerandBabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.Agent System Overview#In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:PlanningSubgoal\n",
            "\n",
            "--- PART 5 ---\n",
            "Tag: p\n",
            "Class: None\n",
            "Content:\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such asAutoGPT,GPT-EngineerandBabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
            "\n",
            "--- PART 6 ---\n",
            "Tag: a\n",
            "Class: None\n",
            "Content:\n",
            "AutoGPT\n",
            "\n",
            "--- PART 7 ---\n",
            "Tag: a\n",
            "Class: None\n",
            "Content:\n",
            "GPT-Engineer\n",
            "\n",
            "--- PART 8 ---\n",
            "Tag: a\n",
            "Class: None\n",
            "Content:\n",
            "BabyAGI\n",
            "\n",
            "--- PART 9 ---\n",
            "Tag: h1\n",
            "Class: None\n",
            "Content:\n",
            "Agent System Overview#\n",
            "\n",
            "--- PART 10 ---\n",
            "Tag: a\n",
            "Class: ['anchor']\n",
            "Content:\n",
            "#\n",
            "\n",
            "--- PART 11 ---\n",
            "Tag: p\n",
            "Class: None\n",
            "Content:\n",
            "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
            "\n",
            "--- PART 12 ---\n",
            "Tag: ul\n",
            "Class: None\n",
            "Content:\n",
            "PlanningSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.MemoryShort-term memory: I would consider all the in-context learning (SeePrompt Engineering) as utilizing short-term memory of the model to learn.Long-\n",
            "\n",
            "--- PART 13 ---\n",
            "Tag: li\n",
            "Class: None\n",
            "Content:\n",
            "PlanningSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
            "\n",
            "--- PART 14 ---\n",
            "Tag: strong\n",
            "Class: None\n",
            "Content:\n",
            "Planning\n",
            "\n",
            "--- PART 15 ---\n",
            "Tag: ul\n",
            "Class: None\n",
            "Content:\n",
            "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s unpack this because it’s *super important* for how the loader → splitter → vector store flow works.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **What actually happens with multiple parts?**\n",
        "\n",
        "When you do this:\n",
        "\n",
        "```python\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs={\"parse_only\": bs4.SoupStrainer(\n",
        "        class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "    )}\n",
        ")\n",
        "docs = loader.load()\n",
        "```\n",
        "\n",
        "👉 **`loader.load()` returns a list of `Document` objects** — *one per matched chunk*.\n",
        "\n",
        "So if your page has:\n",
        "\n",
        "* `<h1 class=\"post-title\"> ... </h1>` → that’s **one Document**\n",
        "* `<header class=\"post-header\"> ... </header>` → another **Document**\n",
        "* `<div class=\"post-content\"> ... </div>` → another **Document**\n",
        "\n",
        "✅ So you may get `len(docs) == 3` for this blog.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 **Do they automatically combine?**\n",
        "\n",
        "**No!** `WebBaseLoader` does not automatically merge them.\n",
        "They stay separate in your `docs` list:\n",
        "\n",
        "```python\n",
        "[\n",
        "    Document(page_content=\"Agents: What are they?...\"),\n",
        "    Document(page_content=\"Author: Lilian Weng...\"),\n",
        "    Document(page_content=\"This post discusses ReAct, ...\")\n",
        "]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **When do they get combined?**\n",
        "\n",
        "They **don’t** get literally merged into one big `Document`.\n",
        "Instead, they go through your **TextSplitter**, which will:\n",
        "\n",
        "* Take each `Document` individually.\n",
        "* Chunk them into smaller overlapping parts.\n",
        "* The result is one long list of **smaller chunks**, all from all the original docs.\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 **Example:**\n",
        "\n",
        "```python\n",
        "# Suppose you have 3 docs:\n",
        "docs = loader.load()\n",
        "print(len(docs))  # 3\n",
        "\n",
        "# Then split:\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "print(len(splits))  # Maybe 20 or 30 chunks!\n",
        "```\n",
        "\n",
        "So in the end:\n",
        "✅ The chunks **represent the combined source**\n",
        "✅ But technically they come from separate source `Documents`\n",
        "✅ Each chunk keeps `metadata` → so your retrieval knows which source it came from!\n",
        "\n",
        "---\n",
        "\n",
        "## 🗂️ **Why this is good**\n",
        "\n",
        "This design means:\n",
        "\n",
        "* You can pull exactly the HTML bits you want → fine control.\n",
        "* You get clear metadata for each chunk.\n",
        "* Your retriever can later show you: *“This passage came from the post-content div, or the title header, etc.”*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MZlHHaX5RdQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(extracted_parts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hApH6u9kQR4s",
        "outputId": "5d5af324-b353-417d-fccc-abcd6b22bc73"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "401"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose you have 3 docs:\n",
        "docs = loader.load()\n",
        "print(len(docs))  # 3\n",
        "\n",
        "# Then split:\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "print(len(splits))  # Maybe 20 or 30 chunks!\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3wEkJoGQR18",
        "outputId": "2f07e7f9-2590-42d0-d152-a8f4dda7770e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_splits[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pxo2nrZ2WXs9",
        "outputId": "026cc959-eb9a-4533-8526-6a4471035cf5"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory')"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splits Inspection\n",
        "\n",
        "✅ **Fantastic insight — you’re asking exactly the right question!**\n",
        "You’re right: the *tidiness* of your final chunks **does directly affect** your RAG output quality. Let’s break down *why*, what’s happening, and how to clean it up if you want **higher-quality chunks**.\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 **What’s in your current split?**\n",
        "\n",
        "Your `all_splits[0]` is:\n",
        "\n",
        "```\n",
        "Document(\n",
        "  metadata={...},\n",
        "  page_content=(\n",
        "    \"LLM Powered Autonomous Agents\\n\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM...\"\n",
        "  )\n",
        ")\n",
        "```\n",
        "\n",
        "So your chunk has:\n",
        "\n",
        "* ✅ **Good content**: the intro, headings, paragraphs.\n",
        "* ⚠️ **Extra noise**: metadata like “Date: … | Author: …” that repeats in other chunks.\n",
        "* ⚠️ Extra `\\n` → doesn’t usually hurt the LLM, but lots of them can waste tokens or break flow.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Why does this matter for RAG?**\n",
        "\n",
        "* The embeddings you generate for similarity search are only as good as the text quality.\n",
        "* If every chunk carries repetitive noise (like the date/author for each blog post), it can:\n",
        "\n",
        "  * Dilute your semantic similarity.\n",
        "  * Waste tokens in your context window when you pass retrieved docs to the LLM.\n",
        "  * Make the final answer sound repetitive or cluttered.\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 **Where does this come from?**\n",
        "\n",
        "* This is from the **HTML structure**:\n",
        "\n",
        "  * For example, the “post-header” block probably includes the title, date, author.\n",
        "  * That’s perfectly fine *once* — but when you split the text, it can get copied into multiple chunks because the `RecursiveCharacterTextSplitter` doesn’t “know” what’s semantically important vs. repeated.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **How tidy is the final product by default?**\n",
        "\n",
        "👉 By default:\n",
        "\n",
        "* It’s **pretty good** — you’re getting meaningful, readable text blocks.\n",
        "* But you *can* get repeated headers or metadata you may not want.\n",
        "* The newline characters `\\n` are usually not a big deal — but for very clean retrieval, you might normalize them.\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡️ **How to clean it up (best practices)**\n",
        "\n",
        "### ✅ 1️⃣ Strip or filter unwanted text before splitting\n",
        "\n",
        "**One-time removal is the best place:**\n",
        "\n",
        "```python\n",
        "# Inspect your docs before splitting:\n",
        "for doc in docs:\n",
        "    print(\"--- RAW DOC ---\")\n",
        "    print(doc.page_content[:500])\n",
        "\n",
        "# Example cleanup:\n",
        "for doc in docs:\n",
        "    text = doc.page_content\n",
        "\n",
        "    # Example: Remove the Date/Author line if it's always the same\n",
        "    text = text.replace(\"Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\", \"\")\n",
        "\n",
        "    # Collapse multiple newlines\n",
        "    text = \"\\n\".join([line.strip() for line in text.splitlines() if line.strip()])\n",
        "\n",
        "    doc.page_content = text\n",
        "```\n",
        "\n",
        "Then split:\n",
        "\n",
        "```python\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "```\n",
        "\n",
        "✅ This means your **chunks will never contain that redundant metadata**, so your embeddings will be clearer.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 2️⃣ Or use a more advanced cleaner\n",
        "\n",
        "You could also:\n",
        "\n",
        "* Use regex to strip repeated footers, headers, disclaimers, timestamps.\n",
        "* Apply `strip()` to remove leading/trailing whitespace.\n",
        "* Remove line breaks if you don’t need them for semantic structure.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 3️⃣ Normalizing whitespace helps\n",
        "\n",
        "LLMs handle `\\n` fine — they even use them in chain-of-thought. But random extra line breaks can waste tokens.\n",
        "One common trick:\n",
        "\n",
        "```python\n",
        "def clean_text(text):\n",
        "    # Remove extra newlines\n",
        "    lines = text.splitlines()\n",
        "    lines = [line.strip() for line in lines if line.strip()]\n",
        "    return \" \".join(lines)\n",
        "\n",
        "for doc in docs:\n",
        "    doc.page_content = clean_text(doc.page_content)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 **Does this really improve RAG performance?**\n",
        "\n",
        "✅ YES — for real-world projects, *cleaner* chunks:\n",
        "\n",
        "* Improve embedding similarity.\n",
        "* Reduce hallucination risk because retrieval is more on-point.\n",
        "* Use your token budget more effectively → less wasted cost.\n",
        "\n",
        "---\n",
        "\n",
        "## 🟢 **So your instinct is 100% right**\n",
        "\n",
        "| Noisy chunks               | Cleaned chunks                    |\n",
        "| -------------------------- | --------------------------------- |\n",
        "| Date/author in every chunk | Appears once, or only in metadata |\n",
        "| Extra whitespace           | Normalized                        |\n",
        "| Poor semantic flow         | Improved relevance                |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ ✅ ✅ **Quick takeaway**\n",
        "\n",
        "* The default tutorial gives you a *decent* baseline.\n",
        "* But a tiny bit of **text pre-processing** can noticeably improve your results — especially for larger RAG projects.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o7G1VvDvTEY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, split in enumerate(all_splits[:10]):  # First 10 splits\n",
        "    print(f\"--- Chunk {i+1} ---\")\n",
        "    print(f\"Length (chars): {len(split.page_content)}\")\n",
        "    print(f\"Metadata: {split.metadata}\")\n",
        "    print(\"\\nCONTENT:\\n\")\n",
        "    print(split.page_content[:500])  # Preview first 500 chars\n",
        "    print('\\n' + '-'*80 + '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSotsy9qUo7I",
        "outputId": "b04de122-b1fa-4850-8eaf-50d43a6c38bc"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Chunk 1 ---\n",
            "Length (chars): 969\n",
            "Metadata: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
            "\n",
            "CONTENT:\n",
            "\n",
            "LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
            "Agent System Overview#\n",
            "In a LLM-p\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Chunk 2 ---\n",
            "Length (chars): 665\n",
            "Metadata: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
            "\n",
            "CONTENT:\n",
            "\n",
            "Memory\n",
            "\n",
            "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
            "Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n",
            "\n",
            "\n",
            "Tool use\n",
            "\n",
            "The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Chunk 3 ---\n",
            "Length (chars): 939\n",
            "Metadata: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
            "\n",
            "CONTENT:\n",
            "\n",
            "Component One: Planning#\n",
            "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
            "Task Decomposition#\n",
            "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Chunk 4 ---\n",
            "Length (chars): 987\n",
            "Metadata: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
            "\n",
            "CONTENT:\n",
            "\n",
            "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
            "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Chunk 5 ---\n",
            "Length (chars): 760\n",
            "Metadata: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
            "\n",
            "CONTENT:\n",
            "\n",
            "Self-Reflection#\n",
            "Self-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\n",
            "ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikip\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Chunk 6 ---\n",
            "Length (chars): 974\n",
            "Metadata: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
            "\n",
            "CONTENT:\n",
            "\n",
            "Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\n",
            "\n",
            "In both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\n",
            "Reflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a stand\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Chunk 7 ---\n",
            "Length (chars): 858\n",
            "Metadata: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
            "\n",
            "CONTENT:\n",
            "\n",
            "Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\n",
            "\n",
            "The heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\n",
            "Self-reflection is created by showing two-shot examples to LLM and each example is a\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Chunk 8 ---\n",
            "Length (chars): 960\n",
            "Metadata: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
            "\n",
            "CONTENT:\n",
            "\n",
            "Chain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\geq r_{n-1} \\geq \\d\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Chunk 9 ---\n",
            "Length (chars): 412\n",
            "Metadata: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
            "\n",
            "CONTENT:\n",
            "\n",
            "To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\n",
            "The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Chunk 10 ---\n",
            "Length (chars): 989\n",
            "Metadata: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
            "\n",
            "CONTENT:\n",
            "\n",
            "After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\n",
            "\n",
            "The idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-con\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Split the text into lines\n",
        "    lines = text.splitlines()\n",
        "\n",
        "    # Strip each line of leading/trailing whitespace and drop empty lines\n",
        "    lines = [line.strip() for line in lines if line.strip()]\n",
        "\n",
        "    # Join the cleaned lines into one string with single spaces\n",
        "    return \" \".join(lines)\n",
        "\n",
        "\n",
        "# Pick a doc to inspect\n",
        "original = docs[0].page_content\n",
        "\n",
        "# Clean it\n",
        "cleaned = clean_text(original)\n",
        "\n",
        "# Compare\n",
        "import textwrap\n",
        "from pprint import pprint  # For pretty-printing dicts if needed\n",
        "\n",
        "# Compare side by side with nice wrapping\n",
        "print(\"--- ORIGINAL ---\")\n",
        "print(\"\\n\".join(textwrap.wrap(original[:500], width=80)))\n",
        "\n",
        "print(\"\\n--- CLEANED ---\")\n",
        "print(\"\\n\".join(textwrap.wrap(cleaned[:500], width=80)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tA9cpLEtT8IR",
        "outputId": "676ffbe2-35b6-4046-f284-8cfd48d601dd"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ORIGINAL ---\n",
            "LLM Powered Autonomous Agents Building agents with LLM (large language model) as\n",
            "its core controller is a cool concept. Several proof-of-concepts demos, such as\n",
            "AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality\n",
            "of LLM extends beyond generating well-written copies, stories, essays and\n",
            "programs; it can be framed as a powerful general problem solver. Agent System\n",
            "Overview# In a LLM-powered autonomous agent system, LLM functions as the agent’s\n",
            "brain, complemented by se\n",
            "\n",
            "--- CLEANED ---\n",
            "LLM Powered Autonomous Agents Building agents with LLM (large language model) as\n",
            "its core controller is a cool concept. Several proof-of-concepts demos, such as\n",
            "AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality\n",
            "of LLM extends beyond generating well-written copies, stories, essays and\n",
            "programs; it can be framed as a powerful general problem solver. Agent System\n",
            "Overview# In a LLM-powered autonomous agent system, LLM functions as the agent’s\n",
            "brain, complemented by se\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt\n",
        "\n",
        "## 📌 **What is `hub.pull(\"rlm/rag-prompt\")` doing?**\n",
        "\n",
        "In the new LangChain 0.2+ framework, **`hub.pull`** loads a prompt **from the LangChain Hub**, which is a shared place to store reusable prompts, chains, and components.\n",
        "\n",
        "```python\n",
        "from langchain import hub\n",
        "\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "```\n",
        "\n",
        "So `rlm/rag-prompt` is:\n",
        "\n",
        "* `rlm` → the username or org on the Hub.\n",
        "* `rag-prompt` → the name of the prompt they published.\n",
        "\n",
        "This lets you reuse **battle-tested** prompt templates instead of writing one from scratch.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Can you see the prompt online?**\n",
        "\n",
        "✔️ Yes! Every public Hub asset has a **web page**.\n",
        "👉 The URL pattern is:\n",
        "\n",
        "```\n",
        "https://smith.langchain.com/hub/<username>/<asset-name>\n",
        "```\n",
        "\n",
        "So in your case:\n",
        "\n",
        "```\n",
        "https://smith.langchain.com/hub/rlm/rag-prompt\n",
        "```\n",
        "\n",
        "If you open that, you’ll see:\n",
        "\n",
        "* The full prompt template.\n",
        "* Inputs it expects (e.g., `{question}` and `{context}`).\n",
        "* Example usage.\n",
        "* Versions, if any.\n",
        "\n",
        "---\n",
        "\n",
        "## 🟢 **What does this prompt contain?**\n",
        "\n",
        "Here’s what a standard **RAG prompt** usually looks like:\n",
        "\n",
        "```\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "If you don’t know the answer, just say you don’t know — don’t try to make up an answer.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Helpful Answer:\n",
        "```\n",
        "\n",
        "✅ It ensures:\n",
        "\n",
        "* The LLM grounds its answer in your retrieved chunks.\n",
        "* It avoids hallucinations by telling the LLM not to guess.\n",
        "* Your `{context}` and `{question}` are plugged in dynamically when you call `prompt.invoke(...)`.\n",
        "\n",
        "---\n",
        "\n",
        "## 🗂️ **How can you inspect it locally?**\n",
        "\n",
        "After pulling it:\n",
        "\n",
        "```python\n",
        "print(prompt)\n",
        "```\n",
        "\n",
        "or:\n",
        "\n",
        "```python\n",
        "print(prompt.messages)\n",
        "```\n",
        "\n",
        "Depending on whether it’s a `PromptTemplate`, `ChatPromptTemplate`, or some other type, you’ll see its structure.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Key takeaway**\n",
        "\n",
        "| Thing                                            | What it does                                                 |\n",
        "| ------------------------------------------------ | ------------------------------------------------------------ |\n",
        "| `hub.pull(\"rlm/rag-prompt\")`                     | Loads a reusable prompt template from the Hub                |\n",
        "| `https://smith.langchain.com/hub/rlm/rag-prompt` | Lets you view the exact prompt                               |\n",
        "| `prompt.invoke({...})`                           | Fills in `{question}` and `{context}` when you run the graph |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ ✅ ✅ **Next**\n",
        "\n",
        "* Go check it out at [smith.langchain.com/hub/rlm/rag-prompt](https://smith.langchain.com/hub/rlm/rag-prompt)\n"
      ],
      "metadata": {
        "id": "kPrUedaieI-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The prompt is a ChatPromptTemplate with one or more messages\n",
        "print(type(prompt))\n",
        "# <class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
        "\n",
        "# The actual list of messages\n",
        "print(prompt.messages)\n",
        "# [HumanMessagePromptTemplate(...), ...]\n",
        "\n",
        "# Grab the first HumanMessagePromptTemplate\n",
        "human_message = prompt.messages[0]\n",
        "\n",
        "print(type(human_message))\n",
        "# <class 'langchain_core.prompts.chat.HumanMessagePromptTemplate'>\n",
        "\n",
        "# That has a .prompt which is your underlying PromptTemplate\n",
        "underlying_template = human_message.prompt\n",
        "\n",
        "print(type(underlying_template))\n",
        "# <class 'langchain_core.prompts.prompt.PromptTemplate'>\n",
        "\n",
        "# Finally, get the raw template string!\n",
        "print('\\n')\n",
        "print(\"\\n\".join(textwrap.wrap(underlying_template.template, width=80)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dpip2TeKT8Fa",
        "outputId": "6e64244b-cf32-454d-8a16-6bdb326269bb"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
            "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n",
            "<class 'langchain_core.prompts.chat.HumanMessagePromptTemplate'>\n",
            "<class 'langchain_core.prompts.prompt.PromptTemplate'>\n",
            "\n",
            "\n",
            "You are an assistant for question-answering tasks. Use the following pieces of\n",
            "retrieved context to answer the question. If you don't know the answer, just say\n",
            "that you don't know. Use three sentences maximum and keep the answer concise.\n",
            "Question: {question}  Context: {context}  Answer:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ **Good RAG prompts are often surprisingly simple.**\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 **Why is it so simple?**\n",
        "\n",
        "Your prompt:\n",
        "\n",
        "```\n",
        "You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use three sentences maximum and keep the answer concise.\n",
        "\n",
        "Question: {question}  \n",
        "Context: {context}  \n",
        "Answer:\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🔑 **What this does right**\n",
        "\n",
        "1️⃣ **Grounds the LLM**\n",
        "\n",
        "* It says: *“Use the context I gave you. Don’t hallucinate.”*\n",
        "\n",
        "2️⃣ **Has a clear fallback**\n",
        "\n",
        "* *“If you don’t know, just say you don’t know.”*\n",
        "* This reduces confident nonsense when the context is irrelevant or missing.\n",
        "\n",
        "3️⃣ **Keeps output short**\n",
        "\n",
        "* *“Three sentences max, keep it concise.”*\n",
        "* Saves tokens and keeps answers direct — especially important if you show the source passages to the user too.\n",
        "\n",
        "4️⃣ **No unnecessary instructions**\n",
        "\n",
        "* No fancy formatting, references, or style — just plain QA.\n",
        "* The retriever does the heavy lifting: the better your chunks, the better this works.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Good RAG = simple prompt + good chunks**\n",
        "\n",
        "You nailed it:\n",
        "\n",
        "* The *real power* comes from **high-quality, well-chunked, relevant context**.\n",
        "* A fancy prompt can’t fix irrelevant or noisy chunks.\n",
        "* A simple, direct prompt works better because the LLM doesn’t waste tokens or logic on unnecessary instructions.\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 **When you might expand it**\n",
        "\n",
        "You’d only make your RAG prompt more complex when you need:\n",
        "\n",
        "* ✅ Citations: *“Add \\[1], \\[2], \\[3] next to facts.”*\n",
        "* ✅ Structured output: JSON, bullet lists, or tables.\n",
        "* ✅ A custom style or tone: e.g., summarizing in plain language for kids.\n",
        "* ✅ Additional constraints: like always adding an explicit source URL.\n",
        "\n",
        "But for baseline QA → **short, direct instructions are king**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🟢 **Key takeaway**\n",
        "\n",
        "| Good chunk quality          | Good retriever                                   | Simple grounding prompt                                |\n",
        "| --------------------------- | ------------------------------------------------ | ------------------------------------------------------ |\n",
        "| ✅ Clean text, minimal noise | ✅ Similarity search that finds relevant passages | ✅ “Use this context. Be concise. Don’t make stuff up.” |\n",
        "\n",
        "This is what gives you reliable, traceable answers.\n",
        "\n"
      ],
      "metadata": {
        "id": "yCQoZIJEf-CU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> *“How important is prepping and optimizing my source documentation for RAG?”*\n",
        "\n",
        "The short answer: **It’s absolutely critical — maybe the single most important thing you control!**\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 **Why your chunk quality matters more than a fancy prompt**\n",
        "\n",
        "### 🗂️ **RAG is really just two big steps:**\n",
        "\n",
        "1️⃣ **Retrieve** — Use embeddings + vector store to find relevant text chunks.\n",
        "2️⃣ **Generate** — Ask the LLM to answer your question using those chunks.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚡ **What does “garbage in, garbage out” mean for RAG?**\n",
        "\n",
        "👉 If your source text is noisy, irrelevant, repetitive, or poorly chunked:\n",
        "\n",
        "* The vector store will return off-topic or low-quality context.\n",
        "* The LLM will either hallucinate to fill gaps **OR** parrot irrelevant details.\n",
        "* No prompt in the world can fix “bad chunks in, bad answer out.”\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **The retrieval step is your “truth filter.”**\n",
        "\n",
        "A clear, tight prompt just says:\n",
        "\n",
        "> “Hey model, trust these chunks. Don’t guess.”\n",
        "\n",
        "But if the chunks themselves are:\n",
        "\n",
        "* Missing key facts\n",
        "* Full of boilerplate (headers, footers, legal disclaimers)\n",
        "* Or too big and vague\n",
        "\n",
        "…the LLM has no good raw material to pull an accurate answer from.\n",
        "\n",
        "---\n",
        "\n",
        "## 🟢 **What makes a chunk “high quality”?**\n",
        "\n",
        "✅ Clear, well-written text — free from extra noise like navbars, unrelated disclaimers.\n",
        "\n",
        "✅ Right chunk size — small enough to stay relevant (\\~500–1000 tokens), large enough to preserve meaning.\n",
        "\n",
        "✅ Overlap for context — to prevent splitting related ideas mid-paragraph.\n",
        "\n",
        "✅ Consistent format — so your embeddings stay semantically sharp.\n",
        "\n",
        "✅ Enriched metadata — so you can filter and display source details later.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧹 **How do people prep their docs well?**\n",
        "\n",
        "📄 **Good examples:**\n",
        "\n",
        "* Strip repeated boilerplate: e.g., headers/footers that repeat every page.\n",
        "* Remove unrelated sections: e.g., site nav, social share buttons.\n",
        "* Normalize whitespace: e.g., clean up newlines, markdown, code blocks.\n",
        "* Keep logical units together: e.g., don’t break a section heading from its paragraph.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **How big is the impact?**\n",
        "\n",
        "There’s tons of evidence (and real-world LangChain teams see this daily!):\n",
        "\n",
        "* Improving chunk quality can improve RAG accuracy dramatically — often more than tweaking your LLM or prompt.\n",
        "* If your chunks are noisy, similarity search pulls bad matches.\n",
        "* Better chunks = better semantic similarity = more relevant context = less hallucination.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔑 **Your secret RAG formula**\n",
        "\n",
        "| Part                          | How much control you have         | Impact      |\n",
        "| ----------------------------- | --------------------------------- | ----------- |\n",
        "| Chunk quality & text cleaning | ✅ 100%                            | 🔥 Massive  |\n",
        "| Chunk size & overlap          | ✅ 100%                            | 🔥 High     |\n",
        "| Prompt clarity                | ✅ 100%                            | 🔥 Medium   |\n",
        "| LLM model choice              | ✅ 50% (API pricing & performance) | 🔥 Moderate |\n",
        "| Vector store tech             | ✅ 100%                            | 🔥 Medium   |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ ✅ ✅ **Key takeaway**\n",
        "\n",
        "> 🏆 *“RAG is mostly about smart retrieval.\n",
        "> Smart retrieval is all about high-quality, clean chunks.”*\n",
        "\n",
        "A fancy prompt is just the icing on the cake.\n",
        "A good retriever *is the cake.* 🍰✨\n"
      ],
      "metadata": {
        "id": "LW6nN0CUiJJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##State\n",
        "\n",
        "This little `State` definition is **super important** for understanding how your LangGraph `StateGraph` works behind the scenes.\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 **What is this doing?**\n",
        "\n",
        "```python\n",
        "from typing_extensions import List, TypedDict\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Define state for application\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    answer: str\n",
        "```\n",
        "\n",
        "**This defines the “shape” of your state:**\n",
        "It’s like saying:\n",
        "\n",
        "> “At any point in this pipeline, my application will carry a `State` dictionary with exactly these keys and types.”\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Why do you need a `State`?**\n",
        "\n",
        "In a LangGraph `StateGraph`:\n",
        "\n",
        "* Your RAG pipeline is built as a series of **steps** (nodes).\n",
        "* Each step **receives** the current `State` and **returns** a modified version of it.\n",
        "* The framework wires these steps together, passing the state along.\n",
        "\n",
        "This gives you:\n",
        "\n",
        "* **Clarity** → you always know what data flows through your app.\n",
        "* **Type safety** → so you don’t accidentally break your chain.\n",
        "* **Composability** → each step only worries about its piece.\n",
        "\n",
        "---\n",
        "\n",
        "## 🟢 **What does each field mean?**\n",
        "\n",
        "| Field      | Type             | What it holds                                                    |\n",
        "| ---------- | ---------------- | ---------------------------------------------------------------- |\n",
        "| `question` | `str`            | The user’s input question.                                       |\n",
        "| `context`  | `List[Document]` | The retrieved chunks from your vector store (before generation). |\n",
        "| `answer`   | `str`            | The final LLM answer.                                            |\n",
        "\n",
        "So your pipeline looks like:\n",
        "\n",
        "```\n",
        "Input: {\"question\": \"What is ReAct?\"}\n",
        "\n",
        "   ↓  retrieve step adds:\n",
        "   {\"question\": \"...\", \"context\": [Document(...), Document(...)]}\n",
        "\n",
        "   ↓  generate step adds:\n",
        "   {\"question\": \"...\", \"context\": [...], \"answer\": \"ReAct is ...\"}\n",
        "```\n",
        "\n",
        "✅ The state grows as each step adds more info.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Why `TypedDict`?**\n",
        "\n",
        "👉 `TypedDict` comes from `typing_extensions` (for backward compatibility with older Python versions).\n",
        "It means:\n",
        "\n",
        "* This `State` is a *dictionary* with a specific schema.\n",
        "* It’s not an actual class with methods — just a *type hint* for your graph.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 **Where does this show up?**\n",
        "\n",
        "```python\n",
        "graph_builder = StateGraph(State)\n",
        "```\n",
        "\n",
        "This means:\n",
        "\n",
        "> “This graph expects every node to handle and return a `State` matching this structure.”\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Key takeaway**\n",
        "\n",
        "| Part        | What it does                                          |\n",
        "| ----------- | ----------------------------------------------------- |\n",
        "| `State`     | Defines the *shared data* passed step-to-step         |\n",
        "| `TypedDict` | Adds type safety and clear contracts                  |\n",
        "| `context`   | Carries retrieved `Document` chunks to your generator |\n",
        "| `answer`    | Carries the final output back to you                  |\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 **So what’s next?**\n",
        "\n",
        "After this, your `retrieve` and `generate` functions:\n",
        "\n",
        "* **Input:** `State`\n",
        "* **Output:** `State` (or a partial dict that the graph merges)\n",
        "\n",
        "✅ It’s clean, predictable, and traceable — perfect for LangSmith to log each step.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ ✅ ✅ **Summary**\n",
        "\n",
        "👉 `State` = your app’s *blueprint for passing info around* in your RAG graph.\n",
        "👉 `TypedDict` makes it explicit and debuggable.\n",
        "👉 You get reproducibility, clarity, and great logs!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t91kruUNjXQz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 📌 **Why do we care about `State`?**\n",
        "\n",
        "When you use a **graph-based pipeline** (like LangGraph), `State` is your **single source of truth** for *what data flows through the system*.\n",
        "\n",
        "It answers:\n",
        "\n",
        "> “What data am I passing from step to step?”\n",
        "> “What did each step add, change, or depend on?”\n",
        "\n",
        "Without a clear `State`, your pipeline is:\n",
        "\n",
        "* 🔥 Harder to debug\n",
        "* 🔥 Easier to break if someone changes one step’s input/output\n",
        "* 🔥 Less traceable in LangSmith or other observability tools\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **What does `State` actually do?**\n",
        "\n",
        "Here’s exactly what it provides:\n",
        "\n",
        "### ✔️ 1️⃣ **Clear data flow**\n",
        "\n",
        "Every node (step) knows:\n",
        "\n",
        "* What it will **get** (`State` input)\n",
        "* What it must **return** (a `State` with the same shape, or a partial update)\n",
        "\n",
        "So there’s no hidden “oh, where did that variable come from?”\n",
        "\n",
        "---\n",
        "\n",
        "### ✔️ 2️⃣ **Makes steps reusable & composable**\n",
        "\n",
        "You can swap out steps (like a new retriever or generator) because:\n",
        "\n",
        "* They speak the same “language” → `State`.\n",
        "* Each step promises: “I’ll take in a `State` and return a valid `State`.”\n",
        "\n",
        "---\n",
        "\n",
        "### ✔️ 3️⃣ **Better debugging & tracing**\n",
        "\n",
        "When you run the pipeline in LangSmith:\n",
        "\n",
        "* You see *exactly* what each node did to the state.\n",
        "* You can inspect the input, output, and intermediate results step-by-step.\n",
        "* This makes finding hallucinations or bad retrieval trivial.\n",
        "\n",
        "---\n",
        "\n",
        "### ✔️ 4️⃣ **Scales for complex workflows**\n",
        "\n",
        "For simple RAG, `State` might just be:\n",
        "\n",
        "```python\n",
        "{\n",
        "  \"question\": \"...\",\n",
        "  \"context\": [...],\n",
        "  \"answer\": \"...\"\n",
        "}\n",
        "```\n",
        "\n",
        "But for real applications, `State` can include:\n",
        "\n",
        "* Retrieval scores\n",
        "* Source docs metadata\n",
        "* Chain-of-thought traces\n",
        "* Evaluation results\n",
        "* User feedback\n",
        "\n",
        "So your whole system becomes:\n",
        "\n",
        "```\n",
        "START\n",
        "  ↓\n",
        "Retrieve chunks → store in `context`\n",
        "  ↓\n",
        "Rerank chunks → update `context` with top 3\n",
        "  ↓\n",
        "Generate → store `answer`\n",
        "  ↓\n",
        "Run eval → store `eval_score`\n",
        "  ↓\n",
        "Save to LangSmith → full `State` trace\n",
        "```\n",
        "\n",
        "All structured, no surprises.\n",
        "\n",
        "---\n",
        "\n",
        "## 🗂️ **Without `State` you’d have spaghetti**\n",
        "\n",
        "Imagine instead:\n",
        "\n",
        "* Each step just returns some raw value.\n",
        "* You’re passing random variables between steps.\n",
        "* Want to add a reranker? You’re untangling function signatures and manually merging variables.\n",
        "\n",
        "Your pipeline gets brittle and confusing really fast.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **State = your contract**\n",
        "\n",
        "LangGraph literally won’t compile the graph if you break the `State` contract.\n",
        "\n",
        "That’s the whole power:\n",
        "\n",
        "> “My pipeline’s data flow is explicit and reliable.”\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡️ **Key takeaway**\n",
        "\n",
        "| ✅ Why `State` exists                                                  | ✅ Why it matters                              |\n",
        "| --------------------------------------------------------------------- | --------------------------------------------- |\n",
        "| Defines what your pipeline inputs, produces, and passes between steps | Makes steps modular, reusable, and safe       |\n",
        "| Enables **step-by-step tracing**                                      | You know *exactly* what happened at each node |\n",
        "| Makes complex flows manageable                                        | Easy to evolve, add steps, or run tests       |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ ✅ ✅ **Bottom line**\n",
        "\n",
        "> 🔑 If you want your RAG pipeline to be **debuggable**, **trustworthy**, and **easy to extend**,\n",
        "> *define your `State` clearly.*\n",
        "\n",
        "It’s one of the biggest upgrades you get when moving from “LLM spaghetti code” to **robust LCEL or LangGraph orchestration.**\n"
      ],
      "metadata": {
        "id": "GQNi5eGorZ6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ **This is where your LangGraph pipeline actually *does the work*!**\n",
        "Let’s break each function down line by line, so you see exactly *how* they transform the `State`.\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 **Big picture**\n",
        "\n",
        "Each function is a **graph step (node)**:\n",
        "\n",
        "* It **takes in** the `State` defined earlier:\n",
        "\n",
        "  ```python\n",
        "  class State(TypedDict):\n",
        "      question: str\n",
        "      context: List[Document]\n",
        "      answer: str\n",
        "  ```\n",
        "* It **returns** a `dict` with one or more keys that merge back into the `State`.\n",
        "\n",
        "LangGraph keeps the `State` flowing through these nodes step-by-step.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **🔹 1️⃣ `retrieve` step**\n",
        "\n",
        "```python\n",
        "def retrieve(state: State):\n",
        "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
        "    return {\"context\": retrieved_docs}\n",
        "```\n",
        "\n",
        "### ✔️ What this does:\n",
        "\n",
        "1️⃣ **Takes the user’s question:**\n",
        "\n",
        "```python\n",
        "state[\"question\"]\n",
        "```\n",
        "\n",
        "This comes from the initial input to the graph:\n",
        "\n",
        "```python\n",
        "graph.invoke({\"question\": \"What are ReAct agents?\"})\n",
        "```\n",
        "\n",
        "2️⃣ **Calls the vector store’s `similarity_search`:**\n",
        "\n",
        "```python\n",
        "retrieved_docs = vector_store.similarity_search(...)\n",
        "```\n",
        "\n",
        "✅ This searches your embeddings for the *most semantically similar chunks*.\n",
        "✅ The result is a `List[Document]`.\n",
        "\n",
        "3️⃣ **Returns an update to the `State`:**\n",
        "\n",
        "```python\n",
        "return {\"context\": retrieved_docs}\n",
        "```\n",
        "\n",
        "✅ LangGraph merges this with the existing state.\n",
        "So after this step:\n",
        "\n",
        "```python\n",
        "{\n",
        "  \"question\": \"What are ReAct agents?\",\n",
        "  \"context\": [Document(...), Document(...)]\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **🔹 2️⃣ `generate` step**\n",
        "\n",
        "```python\n",
        "def generate(state: State):\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"answer\": response.content}\n",
        "```\n",
        "\n",
        "### ✔️ What this does:\n",
        "\n",
        "1️⃣ **Combines retrieved chunks into a single string:**\n",
        "\n",
        "```python\n",
        "docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "```\n",
        "\n",
        "✅ This turns multiple retrieved `Document` objects into a single `context` block to feed the prompt.\n",
        "\n",
        "---\n",
        "\n",
        "2️⃣ **Calls the prompt template to render the final prompt:**\n",
        "\n",
        "```python\n",
        "messages = prompt.invoke({\n",
        "  \"question\": state[\"question\"],\n",
        "  \"context\": docs_content\n",
        "})\n",
        "```\n",
        "\n",
        "✅ This fills in `{question}` and `{context}` → outputs messages for the chat model.\n",
        "\n",
        "---\n",
        "\n",
        "3️⃣ **Calls the LLM to generate the final answer:**\n",
        "\n",
        "```python\n",
        "response = llm.invoke(messages)\n",
        "```\n",
        "\n",
        "✅ This is where the actual answer comes from — using the grounded context.\n",
        "\n",
        "---\n",
        "\n",
        "4️⃣ **Returns an update to the `State`:**\n",
        "\n",
        "```python\n",
        "return {\"answer\": response.content}\n",
        "```\n",
        "\n",
        "✅ So the final `State` now has:\n",
        "\n",
        "```python\n",
        "{\n",
        "  \"question\": \"...\",\n",
        "  \"context\": [...],\n",
        "  \"answer\": \"...\"\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **So what’s the big picture?**\n",
        "\n",
        "| Function   | Input                               | What it adds to `State`          | Output             |\n",
        "| ---------- | ----------------------------------- | -------------------------------- | ------------------ |\n",
        "| `retrieve` | `State` with `question`             | Adds `context` → retrieved docs  | `{\"context\": ...}` |\n",
        "| `generate` | `State` with `question` + `context` | Adds `answer` → final LLM answer | `{\"answer\": ...}`  |\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 **How this connects to the graph**\n",
        "\n",
        "When you do:\n",
        "\n",
        "```python\n",
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "graph = graph_builder.compile()\n",
        "```\n",
        "\n",
        "You’re telling LangGraph:\n",
        "\n",
        "> “Run these nodes in sequence:\n",
        "> START → retrieve → generate.\n",
        "> Keep passing the `State` along and merge the updates at each step.”\n",
        "\n",
        "---\n",
        "\n",
        "## 🟢 **Why is this pattern powerful?**\n",
        "\n",
        "✅ Each step is *tiny*, reusable, and testable on its own.\n",
        "✅ `State` stays explicit — no hidden variables.\n",
        "✅ Everything gets traced to LangSmith → so you see:\n",
        "\n",
        "* What was retrieved\n",
        "* What was generated\n",
        "* Where things went wrong if they do\n",
        "\n"
      ],
      "metadata": {
        "id": "wl3qbBh8sZJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Response\n",
        "\n",
        "✅ This is the **final step** where your whole **LangGraph RAG pipeline** runs!\n",
        "Let’s break down exactly what’s happening when you call:\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
        "print(response[\"answer\"])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 **1️⃣ `graph.invoke(...)`**\n",
        "\n",
        "This kicks off your entire compiled **StateGraph**:\n",
        "\n",
        "```python\n",
        "graph = graph_builder.compile()\n",
        "```\n",
        "\n",
        "Your compiled graph knows:\n",
        "\n",
        "```\n",
        "START → retrieve → generate\n",
        "```\n",
        "\n",
        "So when you call:\n",
        "\n",
        "```python\n",
        "graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
        "```\n",
        "\n",
        "you’re saying:\n",
        "\n",
        "> “Run this graph, starting with this `State`:\n",
        "> `{ \"question\": \"What is Task Decomposition?\" }`”\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **2️⃣ What happens under the hood**\n",
        "\n",
        "**Step-by-step flow:**\n",
        "\n",
        "1️⃣ `State` starts with:\n",
        "\n",
        "```python\n",
        "{\n",
        "  \"question\": \"What is Task Decomposition?\"\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "2️⃣ The **`retrieve` step** runs:\n",
        "\n",
        "```python\n",
        "retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
        "```\n",
        "\n",
        "✅ It finds the most relevant chunks in your indexed docs → updates:\n",
        "\n",
        "```python\n",
        "{\n",
        "  \"question\": \"...\",\n",
        "  \"context\": [Document(...), Document(...)]\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "3️⃣ The **`generate` step** runs:\n",
        "\n",
        "```python\n",
        "docs_content = ...join chunks...\n",
        "messages = prompt.invoke({\"question\": ..., \"context\": ...})\n",
        "response = llm.invoke(messages)\n",
        "```\n",
        "\n",
        "✅ It produces your final answer → updates:\n",
        "\n",
        "```python\n",
        "{\n",
        "  \"question\": \"...\",\n",
        "  \"context\": [...],\n",
        "  \"answer\": \"...\"\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "4️⃣ The final `State` is returned:\n",
        "\n",
        "```python\n",
        "response = {\n",
        "  \"question\": \"...\",\n",
        "  \"context\": [...],\n",
        "  \"answer\": \"...\"\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **3️⃣ `print(response[\"answer\"])`**\n",
        "\n",
        "Finally, you just print the LLM’s generated answer:\n",
        "\n",
        "```python\n",
        "print(response[\"answer\"])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🟢 **What’s actually logged to LangSmith?**\n",
        "\n",
        "Because you have:\n",
        "\n",
        "```python\n",
        "LANGCHAIN_TRACING_V2 = \"true\"\n",
        "LANGCHAIN_API_KEY = ...\n",
        "LANGCHAIN_PROJECT = ...\n",
        "```\n",
        "\n",
        "✅ This entire run is tracked step-by-step:\n",
        "\n",
        "* Input question\n",
        "* Retrieved context chunks (which docs, similarity scores)\n",
        "* Final prompt used for generation\n",
        "* LLM output\n",
        "\n",
        "So you can debug:\n",
        "\n",
        "* Did your retriever find good chunks?\n",
        "* Did the LLM answer accurately?\n",
        "* Did it hallucinate?\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ ✅ ✅ **Key takeaway**\n",
        "\n",
        "| What you did                            | What it means                                                   |\n",
        "| --------------------------------------- | --------------------------------------------------------------- |\n",
        "| `graph.invoke({...})`                   | Runs your entire pipeline                                       |\n",
        "| Starts with `{question: ...}`           | Input state                                                     |\n",
        "| Ends with `{question, context, answer}` | Output state                                                    |\n",
        "| `print(response[\"answer\"])`             | Just shows the final LLM answer — grounded in your vector store |\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 **Next**\n",
        "\n",
        "If you want, you can:\n",
        "✔️ Inspect the entire `response`:\n",
        "\n",
        "```python\n",
        "print(response)\n",
        "```\n",
        "\n",
        "✔️ Loop through `response[\"context\"]` to see what chunks were used:\n",
        "\n",
        "```python\n",
        "for doc in response[\"context\"]:\n",
        "    print(doc.page_content[:200])\n",
        "    print(doc.metadata)\n",
        "```\n",
        "\n",
        "✔️ Or log this to LangSmith and trace every step!\n",
        "\n"
      ],
      "metadata": {
        "id": "hbiVwaxBtOsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
        "print(\"\\n--- RESPONSE ---\")\n",
        "print(\"\\n\".join(textwrap.wrap(response[\"answer\"], width=80)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slHh7PgPrZix",
        "outputId": "42d418c5-1619-4ca9-f6ef-33b7002f449b"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- RESPONSE ---\n",
            "Task decomposition is the process of breaking down a complex task into smaller,\n",
            "manageable steps or subgoals. This can be done using methods like simple\n",
            "prompting, task-specific instructions, or human inputs. Techniques such as Chain\n",
            "of Thought and Tree of Thoughts further enhance this process by facilitating\n",
            "logical reasoning and exploration of multiple possibilities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in response[\"context\"]:\n",
        "    print(doc.page_content[:200])\n",
        "    print(doc.metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ5yAP2LrZf6",
        "outputId": "032231ea-6356-41ef-e84c-8f23b7153f96"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outl\n",
            "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
            "Component One: Planning#\n",
            "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
            "Task Decomposition#\n",
            "Chain of thought (CoT; Wei et al. 2022) has become a s\n",
            "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
            "The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field d\n",
            "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
            "Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\n",
            "\n",
            "The system comprises of 4 stages:\n",
            "(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. Th\n",
            "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trace\n",
        "\n",
        "✅ **Fantastic — this is the perfect next step!**\n",
        "Using **LangSmith** for tracing is how you turn “black box LLM spaghetti” into **clear, debuggable RAG workflows**. Let’s break down exactly **what to look for** when you open your graph run in LangSmith.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 **📌 What your trace shows**\n",
        "\n",
        "When you run:\n",
        "\n",
        "```python\n",
        "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
        "```\n",
        "\n",
        "✅ LangSmith records:\n",
        "\n",
        "1. The **input** `State` (just the `question` at first)\n",
        "2. Each **node** in your graph:\n",
        "\n",
        "   * `retrieve`\n",
        "   * `generate`\n",
        "3. The **intermediate `State`** after each node\n",
        "4. All **calls inside each node**:\n",
        "\n",
        "   * Similarity search: `vector_store.similarity_search(...)`\n",
        "   * Prompt rendering: `prompt.invoke(...)`\n",
        "   * LLM call: `llm.invoke(...)`\n",
        "\n",
        "So you get a **full breadcrumb trail** of how your answer was made.\n",
        "\n",
        "---\n",
        "\n",
        "## 🗂️ **What should you actually check?**\n",
        "\n",
        "Here’s a quick checklist:\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **1️⃣ Look at the input**\n",
        "\n",
        "* Did your question come through exactly as expected?\n",
        "\n",
        "  * e.g., `\"What is Task Decomposition?\"`\n",
        "* Any weird formatting? Extra spaces? Garbage?\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **2️⃣ Look at the `retrieve` node**\n",
        "\n",
        "* **Did it run `similarity_search`?**\n",
        "\n",
        "  * You should see the vector store node.\n",
        "* **What documents did it retrieve?**\n",
        "\n",
        "  * Look for `page_content` of each chunk.\n",
        "  * Did they *actually* mention “Task Decomposition”?\n",
        "* **How many chunks did you get?**\n",
        "\n",
        "  * If you see irrelevant or empty results → maybe your vector store is too small, or your embeddings didn’t index well.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **3️⃣ Look at the `generate` node**\n",
        "\n",
        "* **What was the final prompt?**\n",
        "\n",
        "  * You’ll see exactly how `{question}` and `{context}` were filled in.\n",
        "* **Any redundant info or repeated noise?**\n",
        "\n",
        "  * If your context is cluttered, the LLM might give messy answers.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **4️⃣ Look at the LLM output**\n",
        "\n",
        "* **Does the answer actually use the retrieved context?**\n",
        "\n",
        "  * Or did the model hallucinate details you can’t find in the context?\n",
        "* **Is it concise?**\n",
        "\n",
        "  * Does it respect your prompt instructions (e.g., “Three sentences maximum”)?\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **5️⃣ Look for time & token usage**\n",
        "\n",
        "* For each node, LangSmith shows usage data if available:\n",
        "\n",
        "  * How many tokens did embedding, retrieval, and generation cost?\n",
        "  * Are you paying for extra tokens because of poor chunk prep?\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡️ **Why this matters**\n",
        "\n",
        "This is how you debug real-world RAG systems:\n",
        "\n",
        "* **Good chunks, poor answer?** → Maybe the prompt needs tweaking.\n",
        "* **Poor chunks, decent answer?** → You’re relying on the model’s base knowledge (danger!).\n",
        "* **Nothing retrieved?** → Maybe your chunks don’t cover the query or your embeddings don’t match well.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Example: questions you can answer by tracing**\n",
        "\n",
        "| What to check                      | What it tells you                                     |\n",
        "| ---------------------------------- | ----------------------------------------------------- |\n",
        "| *Which chunks were retrieved?*     | Is retrieval relevant?                                |\n",
        "| *Is context too big or too small?* | Do you need different chunk size/overlap?             |\n",
        "| *Is the LLM ignoring context?*     | Should your prompt be stricter?                       |\n",
        "| *Are costs high?*                  | Can you filter or rerank results to use fewer chunks? |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ ✅ ✅ **Key takeaway**\n",
        "\n",
        "LangSmith’s graph tracing lets you:\n",
        "✔️ Inspect every node in your RAG flow.\n",
        "✔️ Verify that each step does what you expect.\n",
        "✔️ Debug retrieval quality, prompt design, and generation output in a single place.\n",
        "\n",
        "---\n",
        "\n",
        "## 🏆 **Pro tip**\n",
        "\n",
        "Once you trust your trace, you can:\n",
        "\n",
        "* Save example traces to share with teammates.\n",
        "* Run LangSmith **evaluators** that score relevance automatically.\n",
        "* Compare different chunking or embedding models side by side.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FkbfkySRui0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eval"
      ],
      "metadata": {
        "id": "65SU0SH1vEqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -Uq openevals"
      ],
      "metadata": {
        "id": "udqfzNmovIRF"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()  # Uses your LANGCHAIN_API_KEY from your .env\n",
        "\n",
        "# Get your most recent run (you can filter by project too)\n",
        "runs = client.list_runs(project_name=\"project_00\", limit=1)\n",
        "run = list(runs)[0]\n",
        "print(\"Run ID:\", run.id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX8nLjSwrZdG",
        "outputId": "ad60bbbf-6671-471a-860e-abc51d870b7f"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run ID: b4b09c21-9ac2-49bc-84a7-960d571a4b73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openevals.llm import create_llm_as_judge\n",
        "from openevals.prompts import CORRECTNESS_PROMPT\n",
        "\n",
        "# Make a correctness judge\n",
        "correctness_judge = create_llm_as_judge(\n",
        "    prompt=CORRECTNESS_PROMPT,\n",
        "    model=\"openai:o3-mini\",  # Or gpt-4o\n",
        "    feedback_key=\"correctness\",\n",
        ")\n",
        "\n",
        "# Run the eval\n",
        "result = correctness_judge(\n",
        "    inputs={\"question\": \"What is Task Decomposition?\"},\n",
        "    outputs={\"answer\": response[\"answer\"]},\n",
        "    reference_outputs={\"answer\": \"Task Decomposition means breaking a complex task into smaller, manageable sub-tasks.\"},\n",
        ")\n",
        "\n",
        "print(f\"Key: {result['key']}\")\n",
        "print(f\"Score: {result['score']}\")\n",
        "\n",
        "print(\"\\nComment:\")\n",
        "print(\"\\n\".join(textwrap.wrap(result['comment'], width=80)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ChZtsVPvuBT",
        "outputId": "2b2e1638-5555-46c3-84e6-7f50b36d0f92"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key: correctness\n",
            "Score: True\n",
            "\n",
            "Comment:\n",
            "The answer correctly explains that task decomposition involves breaking a\n",
            "complex task into smaller, manageable parts. It is factually accurate and\n",
            "complete, including additional details about methods and techniques that relate\n",
            "to logical reasoning which are consistent with the reference output. Thus, the\n",
            "score should be: true.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 📌 **How the correctness check works**\n",
        "\n",
        "When you run something like:\n",
        "\n",
        "```python\n",
        "from openevals.llm import create_llm_as_judge\n",
        "from openevals.prompts import CORRECTNESS_PROMPT\n",
        "\n",
        "judge = create_llm_as_judge(\n",
        "    prompt=CORRECTNESS_PROMPT,\n",
        "    model=\"openai:o3-mini\",\n",
        "    feedback_key=\"correctness\",\n",
        ")\n",
        "\n",
        "result = judge(\n",
        "    inputs={\"question\": \"...\"},\n",
        "    outputs={\"answer\": \"...\"},\n",
        "    reference_outputs={\"answer\": \"...\"}\n",
        ")\n",
        "```\n",
        "\n",
        "Here’s what happens behind the scenes:\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **1️⃣ The evaluator sets up a “grading rubric”**\n",
        "\n",
        "* `CORRECTNESS_PROMPT` is a clear system prompt template.\n",
        "* It basically says:\n",
        "\n",
        "  > *“Here is the question.\n",
        "  > Here is the model’s answer.\n",
        "  > Here is the reference answer.\n",
        "  > Judge if the answer is factually correct, compared to the reference.”*\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **2️⃣ It calls an LLM to *be the judge***\n",
        "\n",
        "* It passes your `inputs`, `outputs`, and `reference_outputs` to the judge LLM (e.g., `o3-mini` or `gpt-4o`).\n",
        "\n",
        "* The prompt includes clear instructions like:\n",
        "\n",
        "  ```\n",
        "  Score: true if the answer is factually correct and matches the reference.\n",
        "  Provide a concise explanation.\n",
        "  ```\n",
        "\n",
        "* The judge LLM then returns a **structured JSON**:\n",
        "\n",
        "  ```json\n",
        "  {\n",
        "    \"score\": true,\n",
        "    \"comment\": \"This answer correctly explains...\"\n",
        "  }\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **3️⃣ The `score` means pass/fail for factual correctness**\n",
        "\n",
        "* `score: True` means the model’s answer **is aligned with the reference**.\n",
        "* `score: False` means the model added details not present in the reference or got it wrong.\n",
        "\n",
        "So it’s using *another LLM* to compare your RAG output to a trusted answer.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 **So who writes the “reference\\_outputs”?**\n",
        "\n",
        "* **You do** — you provide the “gold standard” expected answer for that question.\n",
        "* This is crucial: if your reference answer is wrong, the eval will be off!\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ ✅ ✅ **So how is correctness judged?**\n",
        "\n",
        "| Step           | What it does                                                       |\n",
        "| -------------- | ------------------------------------------------------------------ |\n",
        "| System prompt  | Instructs the judge LLM on how to compare                          |\n",
        "| Judge LLM      | Reads your model’s answer vs. reference                            |\n",
        "| Grading rubric | Checks for factual match, logical completeness, no hallucinations  |\n",
        "| Output         | `score` True/False and a natural language explanation in `comment` |\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡️ **Why this is so powerful**\n",
        "\n",
        "* It lets you scale up evaluation without human reviewers reading every answer.\n",
        "* You can detect subtle hallucinations: the judge LLM *understands meaning*, not just word overlap.\n",
        "* It works the same whether you’re using GPT-4, Claude, or open-weight models.\n",
        "\n",
        "---\n",
        "\n",
        "## 🏆 **Key takeaway**\n",
        "\n",
        "✅ **Correctness = reference-based eval:** *Did the answer match what it should be?*\n",
        "✅ Uses an LLM-as-a-judge: your reference answer is the source of truth.\n",
        "✅ Super powerful for **automating RAG quality checks**, especially for large datasets.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nilZGhJUxP6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pretty_print_eval_result(result, width=80):\n",
        "    print(f\"Key: {result['key']}\")\n",
        "    print(f\"Score: {result['score']}\\n\")\n",
        "    print(\"Comment:\")\n",
        "    print(\"\\n\".join(textwrap.wrap(result['comment'], width=width)))\n",
        "\n",
        "# Usage:\n",
        "pretty_print_eval_result(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1-G5MBTwgk-",
        "outputId": "671fe1ba-60b2-48c5-8080-ebba728f92bb"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key: correctness\n",
            "Score: True\n",
            "\n",
            "Comment:\n",
            "The answer correctly explains that task decomposition involves breaking a\n",
            "complex task into smaller, manageable parts. It is factually accurate and\n",
            "complete, including additional details about methods and techniques that relate\n",
            "to logical reasoning which are consistent with the reference output. Thus, the\n",
            "score should be: true.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "d139BZVGxd4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    {\n",
        "        \"question\": \"What is Task Decomposition?\",\n",
        "        \"reference_answer\": \"Task Decomposition means breaking a complex task into smaller, manageable subtasks.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are ReAct agents?\",\n",
        "        \"reference_answer\": \"ReAct agents combine reasoning and acting, enabling an agent to reason step-by-step and take actions using tools.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is long-term memory in agents?\",\n",
        "        \"reference_answer\": \"Long-term memory allows an agent to persist information across sessions, often using a vector store for retrieval.\"\n",
        "    },\n",
        "]\n",
        "\n",
        "\n",
        "client = Client()\n",
        "\n",
        "correctness_judge = create_llm_as_judge(\n",
        "    prompt=CORRECTNESS_PROMPT,\n",
        "    model=\"openai:o3-mini\",\n",
        "    feedback_key=\"correctness\"\n",
        ")\n",
        "\n",
        "for item in dataset:\n",
        "    question = item[\"question\"]\n",
        "    ref_answer = item[\"reference_answer\"]\n",
        "\n",
        "    # ✅ Runs for each question\n",
        "    response = graph.invoke({\"question\": question})\n",
        "\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"Model Answer: {response['answer']}\")\n",
        "    print(f\"Reference: {ref_answer}\")\n",
        "\n",
        "    eval_result = correctness_judge(\n",
        "        inputs={\"question\": question},\n",
        "        outputs={\"answer\": response[\"answer\"]},\n",
        "        reference_outputs={\"answer\": ref_answer}\n",
        "    )\n",
        "\n",
        "    print(f\"Score: {eval_result['score']}\")\n",
        "    print(\"\\n\".join(textwrap.wrap(eval_result['comment'], width=80)))\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Optionally log to LangSmith if you want:\n",
        "    runs = client.list_runs(project_name=\"project_00\", limit=1)\n",
        "    run_id = list(runs)[0].id\n",
        "\n",
        "    client.create_feedback(\n",
        "        run_id=run_id,\n",
        "        key=\"correctness\",\n",
        "        score=eval_result[\"score\"],\n",
        "        comment=eval_result[\"comment\"]\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABVC_NLHx7Rr",
        "outputId": "fbe29f90-bafe-4e33-fbf6-af214b084611"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What is Task Decomposition?\n",
            "Model Answer: Task decomposition is the process of breaking down a complex task into smaller, manageable sub-tasks or steps. This can be achieved through various methods, such as prompting a language model with specific instructions, utilizing task-specific guidelines, or incorporating human inputs. Techniques like Chain of Thought and Tree of Thoughts enhance this process by systematically exploring steps and reasoning possibilities.\n",
            "Reference: Task Decomposition means breaking a complex task into smaller, manageable subtasks.\n",
            "Score: True\n",
            "The answer accurately explains that task decomposition involves breaking a\n",
            "complex task into smaller, manageable sub-tasks. It also provides examples of\n",
            "methods such as using language model prompts, task-specific guidelines, human\n",
            "inputs, and references techniques like Chain of Thought and Tree of Thoughts for\n",
            "exploring reasoning and steps. The response is complete, factually correct, and\n",
            "logically consistent with the reference output. Thus, the score should be: true.\n",
            "--------------------------------------------------------------------------------\n",
            "Q: What are ReAct agents?\n",
            "Model Answer: ReAct agents are a type of AI that integrate reasoning and acting processes by extending their action space to include both task-specific actions and language-based reasoning. They utilize a structured prompt format involving \"Thought,\" \"Action,\" and \"Observation\" to facilitate complex decision-making and reasoning tasks. ReAct has demonstrated improved performance over baseline models in various knowledge-intensive and decision-making tasks.\n",
            "Reference: ReAct agents combine reasoning and acting, enabling an agent to reason step-by-step and take actions using tools.\n",
            "Score: True\n",
            "The answer accurately explains that ReAct agents are agents that integrate both\n",
            "reasoning and acting, including a structured process (using terms such as\n",
            "\"Thought,\" \"Action,\" and \"Observation\") for complex decision-making. It also\n",
            "mentions that these agents have shown improved performance over baselines, which\n",
            "is in line with the reference output. The answer provides accurate, complete,\n",
            "and properly detailed information with correct terminology, addressing all parts\n",
            "of the question. Thus, the score should be: true.\n",
            "--------------------------------------------------------------------------------\n",
            "Q: What is long-term memory in agents?\n",
            "Model Answer: Long-term memory in agents refers to the capability of retaining and recalling extensive information over indefinite periods, often through the use of an external vector store that allows for fast retrieval. This memory accommodates the agent's experiences in a structured manner, enabling it to access and leverage past events for future decision-making. Additionally, it synthesizes memories into higher-level inferences to guide future behaviors.\n",
            "Reference: Long-term memory allows an agent to persist information across sessions, often using a vector store for retrieval.\n",
            "Score: True\n",
            "The answer accurately defines long-term memory in agents by explaining its role\n",
            "in retaining information over extended periods, citing the use of an external\n",
            "vector store for fast retrieval, and describing how it organizes experiences to\n",
            "inform future decisions. The explanation is detailed, factually correct, and\n",
            "aligns with the reference output, while expanding on the concept without\n",
            "introducing inaccuracies. Thus, the score should be: true.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> *“Why use LangSmith when I could just run my RAG pipeline and print stuff myself?”*\n",
        "\n",
        "Let’s break down exactly **what LangSmith adds**, so you see when it’s *worth it* — and when you can roll your own.\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 **🔑 The big idea: LangSmith is not just logging**\n",
        "\n",
        "LangSmith is like a **debugger + observability layer + experiment tracker** for LLM apps.\n",
        "\n",
        "It gives you things that would be painful (or impossible) to maintain by hand:\n",
        "\n",
        "* 🔍 **Full step-by-step trace** of every chain, agent, retriever, reranker, prompt, and model call.\n",
        "* 🧵 **Structured `State` tracking** → you always know what data flows through each step.\n",
        "* 🗂️ **Centralized storage of runs**, so you can compare versions.\n",
        "* ✅ **Automated evals** → your correctness and relevance scores live next to the run that produced them.\n",
        "* 🏷️ **Filtering, searching, comparing runs** across experiments.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Here’s what you get that you *don’t* get with just print statements**\n",
        "\n",
        "| 🔍 Feature                        | Without LangSmith                                                | With LangSmith                                                                                 |\n",
        "| --------------------------------- | ---------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- |\n",
        "| 🔗 **End-to-end trace**           | You’d need to manually log each input/output of every function.  | Each node in your graph is automatically traced: input, output, tokens, latency.               |\n",
        "| 📜 **Prompt versioning**          | You’d have to store your prompt templates manually.              | Prompts pulled from LangChain Hub are versioned. You can see what prompt was used for any run. |\n",
        "| 🕵️‍♂️ **Chunk-level visibility** | You’d have to print retrieved chunks to see if retrieval worked. | The retrieved docs, scores, and metadata are visible in the trace.                             |\n",
        "| ⚡ **Eval feedback**               | You’d have to run judges + store results yourself.               | Eval results (correctness, relevance) attach directly to the run for easy comparison.          |\n",
        "| 🗂️ **Experiment tracking**       | You’d need spreadsheets or local logs to compare versions.       | Everything is versioned by project/run ID. You can filter by model, prompt, retriever, etc.    |\n",
        "| ✅ **Shareability**                | You’d need to copy logs for teammates.                           | You get a shareable link for any run with full context.                                        |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Example: A real debugging moment**\n",
        "\n",
        "Imagine you deploy your RAG app and the LLM starts hallucinating.\n",
        "With plain logs:\n",
        "\n",
        "* You see the final answer.\n",
        "* Maybe you see the question.\n",
        "* But do you know:\n",
        "\n",
        "  * What chunks were retrieved?\n",
        "  * Which prompt was used?\n",
        "  * What the retriever scores were?\n",
        "  * Whether the retriever failed or the LLM hallucinated?\n",
        "\n",
        "👉 **In LangSmith, you click into your run:**\n",
        "\n",
        "* Inspect `retrieve` → see exactly which chunks the vector store found.\n",
        "* Inspect `generate` → see the rendered prompt with `{context}`.\n",
        "* Spot: *“Ah! My chunks don’t contain the answer → it’s a retriever problem, not the LLM!”*\n",
        "\n",
        "✅ Saves you hours of guessing.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ ✅ ✅ **When is LangSmith worth it?**\n",
        "\n",
        "| If you’re doing...                     | LangSmith is 🔥                                          |\n",
        "| -------------------------------------- | -------------------------------------------------------- |\n",
        "| Small test notebook                    | Eh — you can just `print()` chunks.                      |\n",
        "| Multi-step RAG graphs                  | Yes! Seeing `State` at each step is huge.                |\n",
        "| Lots of prompt experiments             | Yes — version control + side-by-side runs.               |\n",
        "| Automated evals                        | Yes — scores + traces in one place.                      |\n",
        "| Sharing with your team or stakeholders | Big yes — they can click through what actually happened. |\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡ **What LangSmith doesn’t do**\n",
        "\n",
        "It won’t:\n",
        "\n",
        "* Replace your vector store — it just traces calls to it.\n",
        "* Replace your chunk cleaning — you still do that in your code.\n",
        "* Fix a bad retriever or a noisy corpus.\n",
        "\n",
        "✅ It just makes it crystal clear *why* your LLM did what it did.\n",
        "\n",
        "---\n",
        "\n",
        "## 🟢 **Bottom line**\n",
        "\n",
        "| LangSmith’s job | What it gives you                                      |\n",
        "| --------------- | ------------------------------------------------------ |\n",
        "| Observability   | What data went in/out at each step                     |\n",
        "| Reproducibility | What prompt, model, and retriever version you used     |\n",
        "| Debuggability   | What went wrong — bad chunks? bad generation?          |\n",
        "| Evaluability    | Automatic scoring and feedback that lives with the run |\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 **Key takeaway**\n",
        "\n",
        "👉 For **serious, repeatable RAG** — LangSmith saves you from homegrown logging, tangled notebooks, and “What the heck happened?” moments.\n",
        "\n",
        "For small experiments?\n",
        "Print statements are fine — but you’ll outgrow them fast!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "orVMBWBWy4Jc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S3gtMFrtx7rG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}