{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOQh9zlfaOqyNu4fhPlVoot",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/LangChain/blob/main/LC_007_RAG_PromptTesting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üîç What Is Prompt Engineering?\n",
        "\n",
        "**Prompt engineering** is the art (and science) of crafting the *input* you give a language model to:\n",
        "\n",
        "* Guide its tone, structure, or style\n",
        "* Influence the kind of output you get\n",
        "* Align the model‚Äôs response with your business or task goals\n",
        "\n",
        "At its core, it‚Äôs about **asking the right question, the right way.**\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why It Matters\n",
        "\n",
        "Language models don‚Äôt ‚Äúknow‚Äù what you want unless you tell them. The same input can produce very different results based on:\n",
        "\n",
        "* The **structure** of the prompt\n",
        "* The **persona** or role you assign the model\n",
        "* The **instructions** you embed\n",
        "* The **formatting or examples** you give\n",
        "\n",
        "Even with the same documents in RAG, the **prompt** is what determines how that information is **interpreted, framed, and communicated**.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úçÔ∏è Core Principles of Prompt Engineering\n",
        "\n",
        "### 1. **Clear Role Assignment**\n",
        "\n",
        "Assign a persona that sets tone, audience, and expertise.\n",
        "\n",
        "> *You are a financial advisor helping small Gainesville businesses interpret macroeconomic trends.*\n",
        "\n",
        "### 2. **Explicit Instructions**\n",
        "\n",
        "Tell the model **what to do**, not just what question to answer.\n",
        "\n",
        "> *Identify 3 key trends. Use document metadata. End with a 2-sentence summary.*\n",
        "\n",
        "### 3. **Context Awareness**\n",
        "\n",
        "If you have documents, say **how** to use them.\n",
        "\n",
        "> *Use the following context, citing titles and dates where relevant.*\n",
        "\n",
        "### 4. **Structured Output**\n",
        "\n",
        "Tell the model how to structure the answer.\n",
        "\n",
        "> *Answer in bullet points, each with a heading, explanation, and implication.*\n",
        "\n",
        "### 5. **Few-Shot Examples** (optional)\n",
        "\n",
        "Show 1‚Äì2 examples of what a good output looks like. This is powerful but increases token usage.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yW_Dd4w6ztje"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pip Install Packages"
      ],
      "metadata": {
        "id": "KvNkrKlLz8JB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet \\\n",
        "    langchain \\\n",
        "    langchain-huggingface \\\n",
        "    langchain-openai \\\n",
        "    langchain-community \\\n",
        "    chromadb \\\n",
        "    python-dotenv \\\n",
        "    transformers \\\n",
        "    accelerate \\\n",
        "    sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Iz8hiw3pqrZ",
        "outputId": "aed1c36f-5a3f-4d35-fbf2-0452489c0d3e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Libaries"
      ],
      "metadata": {
        "id": "uBzB4d5mr_BE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üåø Environment setup\n",
        "import os                                 # File paths and OS interaction\n",
        "from dotenv import load_dotenv            # Load environment variables from .env file\n",
        "import langchain; print(langchain.__version__)  # Check LangChain version\n",
        "\n",
        "# üìÑ Document loading and preprocessing\n",
        "from langchain_core.documents import Document                   # Base document type\n",
        "from langchain_community.document_loaders import TextLoader     # Loads plain text files\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Splits long docs into smaller chunks\n",
        "\n",
        "# üî¢ Embeddings + vector storage\n",
        "from langchain_huggingface import HuggingFaceEmbeddings         # HuggingFace embedding model\n",
        "from langchain.vectorstores import Chroma                       # Persistent vector DB (Chroma)\n",
        "\n",
        "# üí¨ Prompting + output\n",
        "from langchain_core.prompts import ChatPromptTemplate           # Chat-style prompt templates\n",
        "from langchain_core.output_parsers import StrOutputParser       # Converts model output to string\n",
        "\n",
        "# üîó Chains / pipelines\n",
        "from langchain_core.runnables import Runnable, RunnableLambda   # Compose custom pipelines\n",
        "\n",
        "# üß† (Optional) Hugging Face LLM client setup\n",
        "# from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace  # For HF inference API\n",
        "\n",
        "# üßæ Pretty printing\n",
        "import textwrap                         # Format long strings for printing\n",
        "from pprint import pprint               # Nicely format nested data structures\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q58WFbWobSR",
        "outputId": "e11c6774-bbcb-4a19-c446-5843647061d8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SET PARAMS"
      ],
      "metadata": {
        "id": "r0YiVfaGBqfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SET MODEL PARAMS\n",
        "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
        "# LLM_MODEL = \"gpt-3.5-turbo\"\n",
        "CHUNK_SIZE = 200\n",
        "CHUNK_OVERLAP = 50\n",
        "K = 2"
      ],
      "metadata": {
        "id": "LEK5hdgCBp_O"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai.chat_models.base import ChatOpenAI\n",
        "\n",
        "LLM_MODEL = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        "    temperature=0.4  # Moderate creativity; adjust as needed\n",
        ")"
      ],
      "metadata": {
        "id": "D24V7GoYtlcm"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üßæ Document Cleaning\n",
        "\n",
        "### üßæ 1. **Load the `.txt` files**\n",
        "\n",
        "We‚Äôll loop through all files in the folder using `TextLoader`.\n",
        "\n",
        "### üßπ 2. **Cleaning**\n",
        "\n",
        "Basic cleaning (e.g. stripping newlines, extra whitespace) is often helpful **before splitting**, especially if the files came from exports or copy-paste.\n",
        "\n",
        "### ‚úÇÔ∏è 3. **Split into chunks**\n",
        "\n",
        "We‚Äôll use `RecursiveCharacterTextSplitter` to chunk documents (typically 500‚Äì1000 characters with slight overlap for context continuity).\n",
        "\n",
        "---\n",
        "\n",
        "### üßº Why Basic Cleaning Helps\n",
        "\n",
        "* Removes linebreaks and blank lines that confuse LLMs\n",
        "* Avoids splitting chunks in weird places\n",
        "* Standardizes format before embedding\n",
        "\n",
        "Later you can add more advanced cleaning (e.g., remove boilerplate, normalize headers), but this is a solid default.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KmSqcFpbs5Ak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load token from .env.\n",
        "load_dotenv(\"/content/API_KEYS.env\", override=True)\n",
        "\n",
        "# Path to your documents\n",
        "docs_path = \"/content/CFFC_docs\"\n",
        "\n",
        "# Step 1: Load all .txt files in the folder\n",
        "raw_documents = []\n",
        "for filename in os.listdir(docs_path):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        file_path = os.path.join(docs_path, filename)\n",
        "        loader = TextLoader(file_path, encoding=\"utf-8\")\n",
        "        docs = loader.load()\n",
        "        raw_documents.extend(docs)\n",
        "\n",
        "print(f\"Loaded {len(raw_documents)} documents.\")\n",
        "\n",
        "# Step 2 (optional): Clean up newlines and extra whitespace\n",
        "def clean_doc(doc: Document) -> Document:\n",
        "    cleaned = \" \".join(doc.page_content.split())  # Removes newlines & extra spaces\n",
        "    return Document(page_content=cleaned, metadata=doc.metadata)\n",
        "\n",
        "cleaned_documents = [clean_doc(doc) for doc in raw_documents]\n",
        "\n",
        "# Step 3: Split documents into chunks\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP\n",
        ")\n",
        "\n",
        "chunked_documents = splitter.split_documents(cleaned_documents)\n",
        "\n",
        "print(f\"Split into {len(chunked_documents)} total chunks.\")\n",
        "\n",
        "# Preview the first 5 chunks\n",
        "print(f\"Showing first 5 of {len(chunked_documents)} chunks:\\n\")\n",
        "\n",
        "for i, doc in enumerate(chunked_documents[:5]):\n",
        "    print(f\"--- Chunk {i+1} ---\")\n",
        "    print(f\"Source: {doc.metadata.get('source', 'N/A')}\\n\")\n",
        "    print(textwrap.fill(doc.page_content[:500], width=100))  # limit preview to 500 characters\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpbJPTIDrVm3",
        "outputId": "869ef4bf-a91f-41e0-85db-1cef44d59bac"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 7 documents.\n",
            "Split into 174 total chunks.\n",
            "Showing first 5 of 174 chunks:\n",
            "\n",
            "--- Chunk 1 ---\n",
            "Source: /content/CFFC_docs/CFFC_What If You Could Cut Cash Flow Forecasting Errors by 50%?.txt\n",
            "\n",
            "Cashflow 4Cast What If You Could Cut Cash Flow Forecasting Errors by 50%? on March 28, 2025 What If\n",
            "You Could Cut Cash Flow Forecasting Errors by 50%? Every business lives or dies by its ability to\n",
            "\n",
            "\n",
            "--- Chunk 2 ---\n",
            "Source: /content/CFFC_docs/CFFC_What If You Could Cut Cash Flow Forecasting Errors by 50%?.txt\n",
            "\n",
            "Every business lives or dies by its ability to manage cash flow. Whether it‚Äôs covering payroll,\n",
            "restocking inventory, or preparing for a seasonal dip ‚Äî having reliable numbers makes all the\n",
            "\n",
            "\n",
            "--- Chunk 3 ---\n",
            "Source: /content/CFFC_docs/CFFC_What If You Could Cut Cash Flow Forecasting Errors by 50%?.txt\n",
            "\n",
            "dip ‚Äî having reliable numbers makes all the difference. And yet, most small business owners are\n",
            "flying blind with clunky spreadsheets or outdated tools that leave them guessing. That‚Äôs where\n",
            "\n",
            "\n",
            "--- Chunk 4 ---\n",
            "Source: /content/CFFC_docs/CFFC_What If You Could Cut Cash Flow Forecasting Errors by 50%?.txt\n",
            "\n",
            "tools that leave them guessing. That‚Äôs where CashFlow4Cast comes in. Using advanced machine\n",
            "learning, we help you cut forecasting errors in half ‚Äî giving you clearer insight, earlier warnings,\n",
            "and\n",
            "\n",
            "\n",
            "--- Chunk 5 ---\n",
            "Source: /content/CFFC_docs/CFFC_What If You Could Cut Cash Flow Forecasting Errors by 50%?.txt\n",
            "\n",
            "giving you clearer insight, earlier warnings, and greater confidence in your day-to-day decisions. üìâ\n",
            "A Real-World Example Here‚Äôs a real forecast from a grocery store chain. First, the Excel-style\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Embed + Persist in Chroma\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hVMiFfU7ulqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Set up Hugging Face embedding model\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
        "\n",
        "# Step 2: Set up Chroma with persistence\n",
        "persist_dir = \"chroma_db\"\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunked_documents,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=persist_dir\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Stored {len(chunked_documents)} chunks in Chroma at '{persist_dir}'\")"
      ],
      "metadata": {
        "id": "2EJR3GwqtXwu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c81ca120-9787-4cfb-db3e-0de78b66a953"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Stored 174 chunks in Chroma at 'chroma_db'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Create the Retriever & Prompt Template"
      ],
      "metadata": {
        "id": "fWnzZXxuwv0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": K})\n",
        "\n",
        "# prompt template\n",
        "prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a helpful assistant that uses business documents to answer questions.\n",
        "Use the following context to answer the question as accurately as possible.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "rY_aJ08FwiX7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Step 3: Create the RAG Chain & Run a Query!"
      ],
      "metadata": {
        "id": "89iAu-wbxB8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define RAG chain\n",
        "rag_chain = (\n",
        "    RunnableLambda(lambda d: {\n",
        "        \"question\": d[\"question\"],\n",
        "        \"docs\": retriever.invoke(d[\"question\"])\n",
        "    })\n",
        "    | RunnableLambda(lambda d: {\n",
        "        \"context\": \"\\n\\n\".join([doc.page_content for doc in d[\"docs\"]]),\n",
        "        \"question\": d[\"question\"]\n",
        "    })\n",
        "    | prompt_template\n",
        "    | LLM_MODEL\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Invoke RAG\n",
        "response = rag_chain.invoke({\n",
        "    \"question\": \"What are the recent economic indicators in Gainesville that affect local businesses?\"\n",
        "})\n",
        "\n",
        "# Print response nicely\n",
        "import textwrap\n",
        "print(\"\\n\" + textwrap.fill(response, width=100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljnbnq2JxKKZ",
        "outputId": "84baf608-ecb0-4422-e42a-ac2a70b41d79"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Some recent economic indicators in Gainesville that can affect local businesses include consumer\n",
            "confidence levels, unemployment rates, housing market trends, and overall economic growth in the\n",
            "region. These factors can impact consumer spending, business investment, and overall business\n",
            "performance in the local economy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### üß© What is `RunnableLambda`?\n",
        "\n",
        "In LangChain, `RunnableLambda` is a utility that lets you wrap **any arbitrary Python function** and plug it into a chain.\n",
        "\n",
        "It's like saying:\n",
        "\n",
        "> ‚ÄúI want to do a little custom logic or data transformation here before moving on to the next step.‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "### üîß Why are we using `RunnableLambda` in your RAG chain?\n",
        "\n",
        "Let‚Äôs break it down:\n",
        "\n",
        "#### ‚úÖ 1. **Retrieve docs**\n",
        "\n",
        "```python\n",
        "RunnableLambda(lambda d: {\n",
        "    \"question\": d[\"question\"],\n",
        "    \"docs\": retriever.invoke(d[\"question\"])\n",
        "})\n",
        "```\n",
        "\n",
        "This part takes the user‚Äôs question (`d[\"question\"]`), uses the retriever to fetch relevant documents, and packages both together to move to the next step.\n",
        "\n",
        "#### ‚úÖ 2. **Format context**\n",
        "\n",
        "```python\n",
        "RunnableLambda(lambda d: {\n",
        "    \"context\": \"\\n\\n\".join([doc.page_content for doc in d[\"docs\"]]),\n",
        "    \"question\": d[\"question\"]\n",
        "})\n",
        "```\n",
        "\n",
        "This part takes the retrieved documents (`d[\"docs\"]`) and **builds a string** from them to pass as the `{context}` variable in the prompt. It also forwards the question.\n",
        "\n",
        "So, this is doing your custom formatting for the prompt.\n",
        "\n",
        "---\n",
        "\n",
        "### üì¶ Summary of the full chain\n",
        "\n",
        "Your RAG chain is basically doing:\n",
        "\n",
        "1. Take the user question.\n",
        "2. Use the retriever to get relevant documents.\n",
        "3. Format those docs into a context string.\n",
        "4. Pass the formatted `context` and `question` into a prompt.\n",
        "5. Send that to the model.\n",
        "6. Parse the model output into plain text.\n",
        "\n",
        "Each `RunnableLambda` gives you the freedom to inject these logic steps **without rewriting the whole chain**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LSxzD17425L2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TESTING"
      ],
      "metadata": {
        "id": "bZcIu2NNNH38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Persona: Goldman Sachs economist"
      ],
      "metadata": {
        "id": "iiCnGYW0vKNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Strategy: Persona-driven\n",
        "prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a Goldman Sachs economist tasked with briefing Gainesville business owners.\n",
        "Use the following economic context to analyze key indicators and explain their impact clearly and concisely.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\")\n",
        "\n",
        "# Define RAG chain\n",
        "rag_chain = (\n",
        "    RunnableLambda(lambda d: {\n",
        "        \"question\": d[\"question\"],\n",
        "        \"docs\": retriever.invoke(d[\"question\"])\n",
        "    })\n",
        "    | RunnableLambda(lambda d: {\n",
        "        \"context\": \"\\n\\n\".join([doc.page_content for doc in d[\"docs\"]]),\n",
        "        \"question\": d[\"question\"]\n",
        "    })\n",
        "    | prompt_template\n",
        "    | LLM_MODEL\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "\n",
        "# Invoke RAG\n",
        "response = rag_chain.invoke({\n",
        "    \"question\": \"What are the recent economic indicators in Gainesville that affect local businesses?\"\n",
        "})\n",
        "\n",
        "# Print response nicely\n",
        "import textwrap\n",
        "print(\"\\n\" + textwrap.fill(response, width=100))"
      ],
      "metadata": {
        "id": "zH_8kD0QschO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2ec43fe-9ed5-4fc3-b7b4-ee99bda05d8d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. Consumer Confidence Index: A positive shift in how people feel about the economy can lead to\n",
            "increased consumer confidence in Gainesville. This can result in higher consumer spending, which\n",
            "benefits local businesses as customers are more willing to make purchases.  2. Unemployment Rate: A\n",
            "decreasing unemployment rate in Gainesville indicates a stronger economy and potentially more\n",
            "disposable income for residents. This can lead to increased demand for goods and services from local\n",
            "businesses.  3. Housing Market Trends: The housing market in Gainesville can also impact local\n",
            "businesses. A booming real estate market can lead to increased construction activity, home sales,\n",
            "and renovations, benefiting industries such as construction, home improvement, and real estate.  4.\n",
            "Business Sentiment: The overall sentiment of businesses in Gainesville can also impact local\n",
            "businesses. Positive sentiment can lead to increased investment, hiring, and expansion, while\n",
            "negative sentiment can result in cautious spending and potential layoffs.  5. Interest Rates:\n",
            "Changes in interest rates can affect borrowing costs for businesses in Gainesville. Lower interest\n",
            "rates can make it cheaper for businesses to borrow money for expansion or investment, while higher\n",
            "rates can increase costs and potentially slow down economic activity.  By monitoring these key\n",
            "economic indicators, Gainesville business owners can better understand the current economic\n",
            "environment and make informed decisions to adapt and thrive in the local market.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining Prompt Features\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ What Is Context Awareness?\n",
        "\n",
        "Context awareness means telling the model:\n",
        "\n",
        "* **What kind of context** it‚Äôs being given (e.g., business reports, meeting notes, technical documents).\n",
        "* **How to use it**, such as analyzing, summarizing, or citing it.\n",
        "* That the context has **metadata** like dates, titles, or types, which can help inform the response.\n",
        "\n",
        "Without context cues, the model may:\n",
        "\n",
        "* Treat the documents like background noise.\n",
        "* Hallucinate or generalize without anchoring in the source.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Why It Matters:\n",
        "\n",
        "* Improves **accuracy** and **relevance**.\n",
        "* Encourages use of **document metadata** to guide or justify responses.\n",
        "* Makes the LLM‚Äôs output feel grounded and **evidence-based**.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ What Is Structured Output?\n",
        "\n",
        "Structured output tells the model:\n",
        "\n",
        "* **How to format** its answer (bullets, tables, numbered points, sections, etc.).\n",
        "* What **parts** or **components** the answer should include.\n",
        "* This is especially useful when your goal is to **compare, summarize, brief, or analyze**.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Why It Matters:\n",
        "\n",
        "* Makes long responses easier to read and extract key info from.\n",
        "* Improves reliability ‚Äî helps avoid vague or meandering outputs.\n",
        "* Useful for **post-processing**, e.g., extracting structured data for UIs, dashboards, or reports.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Kln-JzDG6NiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PROMPT 2"
      ],
      "metadata": {
        "id": "88QaXy7989gt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are an economic analyst preparing a briefing for Gainesville business owners.\n",
        "\n",
        "Analyze the following documents to identify the most relevant local economic indicators.\n",
        "\n",
        "Format your response in three sections:\n",
        "\n",
        "**Overview:** A brief summary of the current economic climate in Gainesville.\n",
        "\n",
        "**Key Indicators:** List 2‚Äì3 indicators. For each, include:\n",
        "- **Indicator Name**\n",
        "- **Explanation** (what‚Äôs happening and what it measures)\n",
        "- **Impact** (how it may affect local businesses)\n",
        "\n",
        "**Recommendations:** Offer 1‚Äì2 pieces of practical advice for how businesses should respond.\n",
        "\n",
        "Keep the tone clear, helpful, and forward-looking. Reference document titles and dates when appropriate.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "# Define RAG chain\n",
        "rag_chain = (\n",
        "    RunnableLambda(lambda d: {\n",
        "        \"question\": d[\"question\"],\n",
        "        \"docs\": retriever.invoke(d[\"question\"])\n",
        "    })\n",
        "    | RunnableLambda(lambda d: {\n",
        "        \"context\": \"\\n\\n\".join([doc.page_content for doc in d[\"docs\"]]),\n",
        "        \"question\": d[\"question\"]\n",
        "    })\n",
        "    | prompt_template\n",
        "    | LLM_MODEL\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "\n",
        "# Invoke RAG\n",
        "response = rag_chain.invoke({\n",
        "    \"question\": \"What are the recent economic indicators in Gainesville that affect local businesses?\"\n",
        "})\n",
        "\n",
        "# Print response nicely\n",
        "import textwrap\n",
        "print(\"\\n\" + textwrap.fill(response, width=100))"
      ],
      "metadata": {
        "id": "WYcQa58uscec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77270856-6298-4ee0-8f52-4a043da9c53f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**Overview:** The current economic climate in Gainesville is experiencing a shift in consumer\n",
            "sentiment towards the economy. This change in perception can have a ripple effect on local\n",
            "businesses, impacting consumer spending and overall business performance.  **Key Indicators:**  1.\n",
            "**Consumer Confidence Index**    - **Explanation:** The Consumer Confidence Index measures how\n",
            "optimistic or pessimistic consumers are about the economy. It is based on surveys that ask consumers\n",
            "about their current and future economic outlook.    - **Impact:** A decrease in consumer confidence\n",
            "can lead to reduced spending, lower demand for goods and services, and decreased revenue for local\n",
            "businesses. On the other hand, an increase in consumer confidence can boost spending and support\n",
            "business growth.  2. **Unemployment Rate**    - **Explanation:** The unemployment rate indicates the\n",
            "percentage of the labor force that is unemployed and actively seeking employment. A high\n",
            "unemployment rate may indicate a weak job market, while a low unemployment rate suggests a strong\n",
            "economy.    - **Impact:** A high unemployment rate can lead to decreased consumer spending, as\n",
            "individuals have less disposable income. This can negatively impact local businesses, especially\n",
            "those in retail and service industries. Conversely, a low unemployment rate can result in increased\n",
            "consumer spending and business activity.  **Recommendations:** 1. Monitor Consumer Confidence: Stay\n",
            "informed about changes in consumer sentiment by regularly checking the Consumer Confidence Index.\n",
            "Adjust marketing strategies and inventory levels based on shifts in consumer confidence to better\n",
            "meet customer demand.     2. Diversify Revenue Streams: In response to economic uncertainty,\n",
            "consider diversifying revenue streams to reduce reliance on any single source of income. Explore new\n",
            "markets, products, or services that can help mitigate the impact of economic fluctuations on your\n",
            "business.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PROMPT 3"
      ],
      "metadata": {
        "id": "8Pjqtz1S85-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a local economic analyst preparing a practical briefing for Gainesville business owners.\n",
        "Keep your tone clear, confident, and accessible ‚Äî like you're talking to a room of experienced entrepreneurs.\n",
        "\n",
        "Format your response in **Markdown** using the following structure:\n",
        "\n",
        "## üìä Overview\n",
        "_A brief summary of the current economic climate._\n",
        "\n",
        "## üîç Key Indicators\n",
        "### 1. **[Indicator Name]**\n",
        "- **Explanation:** What the indicator is and what it measures.\n",
        "- **Impact:** How this affects local businesses.\n",
        "\n",
        "### 2. **[Second Indicator]**\n",
        "- ...\n",
        "\n",
        "## üí° Recommendations\n",
        "- Use concise bullet points.\n",
        "- Give actionable advice based on the indicators.\n",
        "\n",
        "---\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "# Define RAG chain\n",
        "rag_chain = (\n",
        "    RunnableLambda(lambda d: {\n",
        "        \"question\": d[\"question\"],\n",
        "        \"docs\": retriever.invoke(d[\"question\"])\n",
        "    })\n",
        "    | RunnableLambda(lambda d: {\n",
        "        \"context\": \"\\n\\n\".join([doc.page_content for doc in d[\"docs\"]]),\n",
        "        \"question\": d[\"question\"]\n",
        "    })\n",
        "    | prompt_template\n",
        "    | LLM_MODEL\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "\n",
        "# Invoke RAG\n",
        "response = rag_chain.invoke({\n",
        "    \"question\": \"What are the recent economic indicators in Gainesville that affect local businesses?\"\n",
        "})\n",
        "\n",
        "# Print response nicely\n",
        "# import textwrap\n",
        "# print(\"\\n\" + textwrap.fill(response, width=100))\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2UI2SY88qJZ",
        "outputId": "5af12269-2907-4005-8c34-a3c3a24b3298"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## üìä Overview\n",
            "The current economic climate in Gainesville is showing signs of stability and growth, with consumer confidence on the rise and unemployment rates decreasing.\n",
            "\n",
            "## üîç Key Indicators\n",
            "### 1. **Consumer Confidence**\n",
            "- **Explanation:** Consumer confidence measures how optimistic or pessimistic consumers are about the state of the economy and their personal financial situation.\n",
            "- **Impact:** A high level of consumer confidence typically leads to increased spending, which can benefit local businesses by boosting sales.\n",
            "\n",
            "### 2. **Unemployment Rate**\n",
            "- **Explanation:** The unemployment rate indicates the percentage of the labor force that is unemployed and actively seeking employment.\n",
            "- **Impact:** A decreasing unemployment rate suggests a stronger job market, which can result in higher consumer spending and a larger customer base for local businesses.\n",
            "\n",
            "## üí° Recommendations\n",
            "- Monitor consumer confidence trends and adjust marketing strategies accordingly to capitalize on increased spending.\n",
            "- Take advantage of a stronger job market by actively recruiting and retaining top talent to support business growth.\n",
            "- Stay informed about economic indicators and adjust business strategies accordingly to stay ahead of the curve.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### üîç Anticipated Visitor Questions\n",
        "\n",
        "1. **Understanding the Service:**\n",
        "\n",
        "   * *What is Cashflow 4Cast, and how does it differ from traditional forecasting methods?*\n",
        "   * *How does AI enhance cash flow forecasting accuracy?*\n",
        "\n",
        "2. **Implementation & Integration:**\n",
        "\n",
        "   * *How can I integrate Cashflow 4Cast with my existing financial systems?*\n",
        "   * *What is the setup process, and how long does it take?*\n",
        "\n",
        "3. **Benefits & Outcomes:**\n",
        "\n",
        "   * *What tangible benefits can I expect from using Cashflow 4Cast?*\n",
        "   * *Are there case studies or testimonials from similar businesses?*\n",
        "\n",
        "4. **Customization & Flexibility:**\n",
        "\n",
        "   * *Can the forecasting models be tailored to my specific industry or business model?*\n",
        "   * *How does the system handle unique financial events or anomalies?*\n",
        "\n",
        "5. **Support & Resources:**\n",
        "\n",
        "   * *What kind of customer support is available?*\n",
        "   * *Are there tutorials or guides to help me get started?*\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Enhancing Your RAG Pipeline\n",
        "\n",
        "To effectively address these queries:\n",
        "\n",
        "* **Document Selection:** Ensure that your RAG system prioritizes documents from the \"Welcome\" section, as they directly address the foundational aspects of your service.\n",
        "\n",
        "* **Metadata Utilization:** Incorporate metadata such as publication dates and titles to provide context and credibility to the retrieved information.\n",
        "\n",
        "* **Prompt Engineering:** Craft prompts that guide the AI to extract and present information in a user-friendly manner. For example:\n",
        "\n",
        "  ```python\n",
        "  prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
        "  You are an AI assistant for Cashflow 4Cast, helping users understand and utilize AI-powered cash flow forecasting.\n",
        "\n",
        "  Context:\n",
        "  {context}\n",
        "\n",
        "  Question:\n",
        "  {question}\n",
        "\n",
        "  Answer:\n",
        "  \"\"\")\n",
        "  ```\n",
        "\n",
        "* **Structured Responses:** Encourage the AI to present answers with clear headings and bullet points, making it easier for users to digest information.\n",
        "\n",
        "---\n",
        "\n",
        "### üìà Future Enhancements\n",
        "\n",
        "As you expand your RAG system to include broader economic data:\n",
        "\n",
        "* **Segmented Pipelines:** Maintain separate pipelines for company-specific content and general economic indicators to ensure relevance and accuracy.\n",
        "\n",
        "* **User Intent Detection:** Implement mechanisms to discern user intent, directing queries to the appropriate pipeline based on whether they're seeking information about your services or broader economic insights.\n",
        "\n",
        "* **Continuous Learning:** Regularly update your document corpus with new blog posts, articles, and user feedback to keep the RAG system current and responsive.\n",
        "\n",
        "---\n",
        "\n",
        "By aligning your RAG pipeline with the specific needs and questions of your website visitors, you can enhance user engagement, provide valuable insights, and position Cashflow 4Cast as a trusted resource in AI-driven financial forecasting. If you need assistance in implementing these suggestions or have further questions, feel free to ask!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rO_Fmv9iD7mm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XvbvR4XH8rtU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}