{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNh7K/HEDdwTuRGgnG8GNeu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/RAG-LangChain/blob/main/Copy_of_LC_017_RAG_RetrieverEVAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pip Installs\n"
      ],
      "metadata": {
        "id": "bRxKQ01_5qiu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaoy9xWg5jcA"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet \\\n",
        "    langchain \\\n",
        "    langchain-huggingface \\\n",
        "    langchain-openai \\\n",
        "    langchain-community \\\n",
        "    chromadb \\\n",
        "    python-dotenv \\\n",
        "    transformers \\\n",
        "    accelerate \\\n",
        "    sentencepiece \\\n",
        "    ragas datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Libraries"
      ],
      "metadata": {
        "id": "dQpA3LYl5xeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🌿 Environment setup\n",
        "import os                                 # File paths and OS interaction\n",
        "from dotenv import load_dotenv            # Load environment variables from .env file\n",
        "import langchain; print(langchain.__version__)  # Check LangChain version\n",
        "import itertools\n",
        "\n",
        "# 📄 Document loading and preprocessing\n",
        "from langchain_core.documents import Document                   # Base document type\n",
        "from langchain_community.document_loaders import TextLoader     # Loads plain text files\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Splits long docs into smaller chunks\n",
        "\n",
        "# 🔢 Embeddings + vector storage\n",
        "from langchain_huggingface import HuggingFaceEmbeddings         # HuggingFace embedding model\n",
        "from langchain.vectorstores import Chroma                       # Persistent vector DB (Chroma)\n",
        "import chromadb\n",
        "\n",
        "# 💬 Prompting + output\n",
        "from langchain_openai.chat_models.base import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate           # Chat-style prompt templates\n",
        "from langchain_core.output_parsers import StrOutputParser       # Converts model output to string\n",
        "\n",
        "# 🔗 Chains / pipelines\n",
        "from langchain_core.runnables import Runnable, RunnableLambda   # Compose custom pipelines\n",
        "\n",
        "# 🧠 (Optional) Hugging Face LLM client setup\n",
        "# from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace  # For HF inference API\n",
        "\n",
        "# 🧾 Pretty printing\n",
        "import textwrap                         # Format long strings for printing\n",
        "from pprint import pprint               # Nicely format nested data structures\n",
        "\n",
        "# Pydantic\n",
        "from pydantic import BaseModel, ValidationError"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4D0q-V9o5qEY",
        "outputId": "435a5dde-97ff-4c1e-b93b-06ef383bd0bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Params"
      ],
      "metadata": {
        "id": "XX3wlWfU54-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load API key\n",
        "from openai import OpenAI\n",
        "\n",
        "# Load token from .env.\n",
        "load_dotenv(\"/content/API_KEYS.env\", override=True)\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# SET MODEL PARAMS\n",
        "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
        "CHUNK_SIZE = 200\n",
        "CHUNK_OVERLAP = 50\n",
        "K = 2\n",
        "\n",
        "LLM_MODEL = ChatOpenAI(\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    temperature=0.4  # Moderate creativity; adjust as needed\n",
        ")"
      ],
      "metadata": {
        "id": "gqOaCxGN54B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load & Clean Docs"
      ],
      "metadata": {
        "id": "ueV1ZyJo594z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Why We Tag Chunks with `source` Metadata\n",
        "\n",
        "In a Retrieval-Augmented Generation (RAG) system, it’s critical to know **where each chunk comes from** so you can measure how well your retriever works.  \n",
        "By adding a `source` tag (the filename) to each document’s metadata before splitting it into chunks, we:\n",
        "\n",
        "- ✅ Preserve the connection between each chunk and its original document\n",
        "- ✅ Make it possible to check if the retriever returned the correct chunks for a given question\n",
        "- ✅ Enable meaningful metrics like **Recall@k**, **Precision@k**, and **F1@k**\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Why We Calculate Retrieval Metrics\n",
        "\n",
        "Good retrieval is the backbone of a good RAG system.  \n",
        "**Precision and Recall** help us understand:\n",
        "- **Recall@k:** *Did the retriever return all the relevant chunks?*  \n",
        "- **Precision@k:** *Are the retrieved chunks actually relevant, or is there extra noise?*  \n",
        "- **F1@k:** *Balances precision and recall into a single score.*\n",
        "\n",
        "High recall means we rarely miss relevant info; high precision means we avoid irrelevant context that might confuse the LLM.  \n",
        "Without these metrics, you can’t tell if poor answers come from bad retrieval or bad generation.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ How This Works in Our Pipeline\n",
        "\n",
        "- When we chunk each document, we add its filename to `metadata[\"source\"]`.\n",
        "- When we evaluate, we compare the **retrieved chunks’ sources** to the **expected sources** we define in our gold-standard QA dataset.\n",
        "- This lets us calculate retrieval metrics programmatically, track weak spots, and improve both chunking and retriever settings over time.\n",
        "\n",
        "**Result:** A robust, trustworthy RAG system that retrieves relevant information and generates factual answers.\n"
      ],
      "metadata": {
        "id": "2c-P1jyzFtDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to documents\n",
        "docs_path = \"/content/CFFC\"\n",
        "\n",
        "# Step 1: Load all .txt files in the folder\n",
        "raw_documents = []\n",
        "\n",
        "for filename in os.listdir(docs_path):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        file_path = os.path.join(docs_path, filename)\n",
        "        loader = TextLoader(file_path, encoding=\"utf-8\")\n",
        "        docs = loader.load()\n",
        "\n",
        "        for doc in docs:\n",
        "            doc.metadata[\"source\"] = filename   # <<--- add file name!\n",
        "            raw_documents.append(doc)\n",
        "\n",
        "\n",
        "print(f\"Loaded {len(raw_documents)} documents.\")\n",
        "\n",
        "# Step 2 (optional): Clean up newlines and extra whitespace\n",
        "def clean_doc(doc: Document) -> Document:\n",
        "    cleaned = \" \".join(doc.page_content.split())  # Removes newlines & extra spaces\n",
        "    return Document(page_content=cleaned, metadata=doc.metadata)\n",
        "\n",
        "cleaned_documents = [clean_doc(doc) for doc in raw_documents]\n",
        "\n",
        "# Step 3: Split documents into chunks\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP\n",
        ")\n",
        "\n",
        "chunked_documents = splitter.split_documents(cleaned_documents)\n",
        "\n",
        "print(f\"Split into {len(chunked_documents)} total chunks.\")\n",
        "\n",
        "# Preview the first 5 chunks\n",
        "print(f\"Showing first 5 of {len(chunked_documents)} chunks:\\n\")\n",
        "\n",
        "for i, doc in enumerate(chunked_documents[:5]):\n",
        "    print(f\"--- Chunk {i+1} ---\")\n",
        "    print(f\"Source: {doc.metadata.get('source', 'N/A')}\\n\")\n",
        "    print(textwrap.fill(doc.page_content[:500], width=100))  # limit preview to 500 characters\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZfRS0lKEwMS",
        "outputId": "2f6e3738-6a20-4a1a-c0e2-b719cb0f7692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 5 documents.\n",
            "Split into 83 total chunks.\n",
            "Showing first 5 of 83 chunks:\n",
            "\n",
            "--- Chunk 1 ---\n",
            "Source: CFFC_Proof_MultiStoreAccuracy.txt\n",
            "\n",
            "# 📊 Consistency That Builds Confidence Forecasting accuracy isn’t just about one-time wins — it’s\n",
            "about **reliable, repeatable performance** across your business. Cashflow4cast delivers consistent\n",
            "\n",
            "\n",
            "--- Chunk 2 ---\n",
            "Source: CFFC_Proof_MultiStoreAccuracy.txt\n",
            "\n",
            "your business. Cashflow4cast delivers consistent forecasting accuracy, store after store. --- ## 🏪\n",
            "Proven Across 20+ Locations We tested our machine learning model across **20+ different store\n",
            "\n",
            "\n",
            "--- Chunk 3 ---\n",
            "Source: CFFC_Proof_MultiStoreAccuracy.txt\n",
            "\n",
            "learning model across **20+ different store locations**, each with unique demand patterns.\n",
            "**Result:** In every case, the ML model **reduced forecasting errors by at least 50%**,\n",
            "outperforming\n",
            "\n",
            "\n",
            "--- Chunk 4 ---\n",
            "Source: CFFC_Proof_MultiStoreAccuracy.txt\n",
            "\n",
            "errors by at least 50%**, outperforming traditional Excel-style models (including Prophet). --- ## 📉\n",
            "Error Reduction: ML vs. Traditional Forecasting - **Traditional Tools (Excel / Prophet):** -\n",
            "\n",
            "\n",
            "--- Chunk 5 ---\n",
            "Source: CFFC_Proof_MultiStoreAccuracy.txt\n",
            "\n",
            "- **Traditional Tools (Excel / Prophet):** - Static, lagging predictions - High error rates,\n",
            "especially with demand shifts - **ML-Based Forecasting:** - Adapts to each location’s patterns -\n",
            "Maintains\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ What Does the `@k` Mean in Retrieval Metrics?\n",
        "\n",
        "When you see metrics like **Recall@k** or **Precision@k**, the **`@k`** (read as “at k”) means we’re calculating the metric based on the **top *k* chunks** returned by the retriever.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 Why This Matters\n",
        "\n",
        "In a Retrieval-Augmented Generation (RAG) system:\n",
        "- The retriever pulls the *k* most relevant chunks for a given question.\n",
        "- The LLM uses only these chunks to generate its final answer.\n",
        "- If the right information isn’t in the top *k*, the answer quality suffers — so we need to know how well the retriever does *within that limit*.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ What Recall@k Means\n",
        "\n",
        "- **Recall@k** measures how many of the *relevant* chunks were successfully retrieved in the top *k*.\n",
        "- Example: If 2 chunks are truly relevant and both are in the top 5 → Recall@5 = 1.0 (100%).\n",
        "- If only 1 is found → Recall@5 = 0.5 (50%).\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ What Precision@k Means\n",
        "\n",
        "- **Precision@k** measures how many of the top *k* retrieved chunks are actually relevant.\n",
        "- Example: If the retriever returns 5 chunks and only 2 are correct → Precision@5 = 0.4 (40%).\n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 Why `@k` Is Critical for RAG\n",
        "\n",
        "- It reflects how well your retriever performs given the constraints your LLM works with.\n",
        "- Tuning *k* helps you balance **recall** (finding all useful info) vs **precision** (avoiding irrelevant info that might confuse the model).\n",
        "\n",
        "---\n",
        "\n",
        "**Key takeaway:**  \n",
        "Adding `@k` makes your retrieval evaluation realistic and reproducible — you know exactly how deep your retriever had to go to find the right context!\n"
      ],
      "metadata": {
        "id": "EiMm2QXwGuYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embed & Persist"
      ],
      "metadata": {
        "id": "Ex-1g4j_6CqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Set up Hugging Face embedding model\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
        "\n",
        "# Step 2: Create your Chroma client with telemetry OFF\n",
        "persist_dir = \"chroma_db\"\n",
        "\n",
        "client = chromadb.PersistentClient(\n",
        "    path=persist_dir,\n",
        "    settings=chromadb.config.Settings(\n",
        "        anonymized_telemetry=False\n",
        "    )\n",
        ")\n",
        "\n",
        "# Step 3: Build vector store using your client\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunked_documents,\n",
        "    embedding=embedding_model,\n",
        "    client=client,\n",
        "    persist_directory=persist_dir\n",
        ")\n",
        "\n",
        "print(f\"✅ Stored {len(chunked_documents)} chunks in Chroma at '{persist_dir}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDI07Zcv6CHg",
        "outputId": "3a8070f4-6333-4756-d517-7b0d1d8caf66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Stored 83 chunks in Chroma at 'chroma_db'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retriever Prompt\n",
        "\n",
        "### ✅ How `k` Affects Answer Quality\n",
        "\n",
        "- **Higher `k`** → More context chunks → Higher chance all relevant info is included → But can make answers longer, repetitive, or off-topic if irrelevant chunks are pulled.\n",
        "- **Lower `k`** → Fewer chunks → Shorter, more focused input → But you risk missing important details if the retriever doesn't find everything.\n",
        "- This is why tuning `k` and measuring **Recall@k** and **Precision@k** are critical: they help you balance *coverage* vs *focus* to get accurate, concise answers.\n"
      ],
      "metadata": {
        "id": "98suZHyf6JXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": K})\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a helpful assistant that uses business documents to answer questions.\n",
        "Use the following context to answer the question as accurately as possible.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\")\n",
        "\n",
        "def generate_answer(question: str):\n",
        "    \"\"\"\n",
        "    1. Retrieve K chunks\n",
        "    2. Concatenate into a single context string\n",
        "    3. Feed context + question to the generator LLM\n",
        "    4. Return (answer, context)   <-- key!\n",
        "    \"\"\"\n",
        "    # Step-1 ➜ retrieve top K document chunks\n",
        "    docs = retriever.invoke(question)\n",
        "    retrieved_ids = [doc.metadata[\"source\"] for doc in docs] # doc ids are now included\n",
        "\n",
        "\n",
        "    # Step-2 ➜ combine those into a context string\n",
        "    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    # Step-3 ➜ fill the prompt template with context and question\n",
        "    llm_input = prompt_template.format(context=context, question=question)\n",
        "\n",
        "    # Step-4 ➜ call the LLM and get its plain text output\n",
        "    answer = StrOutputParser().invoke(LLM_MODEL.invoke(llm_input))\n",
        "\n",
        "    # Step-5 ➜ return the answer AND the context used\n",
        "    return answer, context\n"
      ],
      "metadata": {
        "id": "1q0KBEdX6IrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define RAG chain"
      ],
      "metadata": {
        "id": "ZunJ9Pln6N_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain = (\n",
        "    RunnableLambda(lambda d: {\n",
        "        \"question\": d[\"question\"],\n",
        "        \"docs\": retriever.invoke(d[\"question\"])\n",
        "    })\n",
        "    | RunnableLambda(lambda d: {\n",
        "        \"context\": \"\\n\\n\".join([doc.page_content for doc in d[\"docs\"]]),\n",
        "        \"sources\": [doc.metadata[\"source\"] for doc in d[\"docs\"]],\n",
        "        \"question\": d[\"question\"]\n",
        "    })\n",
        "    | RunnableLambda(lambda d: {\n",
        "        \"answer\": LLM_MODEL.invoke(prompt_template.format(\n",
        "            context=d[\"context\"], question=d[\"question\"]\n",
        "        )).content,\n",
        "        \"sources\": d[\"sources\"]\n",
        "    })\n",
        ")\n",
        "\n",
        "# Invoke RAG\n",
        "response = rag_chain.invoke({\n",
        "    \"question\": \"What are the recent economic indicators in Gainesville that affect local businesses?\"\n",
        "})\n",
        "\n",
        "# Print response nicely\n",
        "print(\"\\n\" + textwrap.fill(response[\"answer\"], width=100))\n",
        "print(\"\\nRetrieved sources:\", response[\"sources\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2XX2iF36NW0",
        "outputId": "d4755e23-0a90-4040-85e3-adca8e49e4df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The provided context does not include specific recent economic indicators for Gainesville or any\n",
            "local data. To accurately answer your question about recent economic indicators in Gainesville that\n",
            "affect local businesses, you would need to refer to local economic reports, government publications,\n",
            "or business surveys that detail the current economic conditions in that area. Typically, indicators\n",
            "such as unemployment rates, consumer spending, housing market trends, and local business growth\n",
            "rates are relevant.\n",
            "\n",
            "Retrieved sources: ['/content/CFFC/CFFC_EconomicIndicators_Federal.txt', '/content/CFFC/CFFC_EconomicIndicators_Federal.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Why the LLM Sometimes Says “No Data in Context”\n",
        "\n",
        "In a Retrieval-Augmented Generation (RAG) pipeline:\n",
        "- The LLM can only answer questions using the **retrieved context**.\n",
        "- If the retriever does not find chunks that contain the specific fact needed, the LLM must rely only on what it sees.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 Why This Happens\n",
        "\n",
        "When the knowledge base does **not contain** the information a user is asking for (e.g., local economic indicators for Gainesville):\n",
        "- The retriever pulls the closest matching chunks it can find (in this case, federal indicators).\n",
        "- The LLM reads the context and realizes the specific detail is missing.\n",
        "- The LLM correctly returns an answer like:\n",
        "  > “The provided context does not include this information…”\n",
        "\n",
        "This is exactly how a well-designed RAG system should behave — it avoids **hallucinating** or making up facts.\n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 What This Tells Us\n",
        "\n",
        "- **Recall@k = 0:** The relevant information does not exist in the top *k* retrieved chunks — because it doesn’t exist in the knowledge base at all.\n",
        "- **Precision@k:** May also be low, since the retrieved chunks are only partially relevant.\n",
        "- This signals a **content gap**, not a technical failure. The pipeline worked properly!\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ What To Do About It\n",
        "\n",
        "- Keep these “no result” queries in your test set — they highlight real **knowledge gaps**.\n",
        "- If these topics matter to your users (e.g., local Gainesville stats), add trusted local data to your corpus.\n",
        "- This improves your retrieval recall in the future — because the fact now exists to be found.\n",
        "\n",
        "---\n",
        "\n",
        "**Key takeaway:**  \n",
        "A fallback answer means your RAG system is trustworthy.  \n",
        "You should fill content gaps — not force the LLM to guess.\n"
      ],
      "metadata": {
        "id": "QpQJleUNZ2Cc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Adding Missing Local Data: What We Learned\n",
        "\n",
        "### 🔍 How We Discovered the Gap\n",
        "\n",
        "During testing, we asked our RAG pipeline:\n",
        "> \"What are the recent economic indicators in Gainesville that affect local businesses?\"\n",
        "\n",
        "The system returned:\n",
        "> \"The provided context does not include specific recent economic indicators for Gainesville...\"\n",
        "\n",
        "This meant:\n",
        "- The retriever did not return relevant local chunks.\n",
        "- Checking our vector database showed that we never embedded our Gainesville-specific content — even though this info *did exist* on our website.\n",
        "\n",
        "---\n",
        "\n",
        "### 🗝️ Lesson Learned\n",
        "\n",
        "**A RAG pipeline can only retrieve what’s in the knowledge base.**  \n",
        "If a user asks about a topic not in your corpus, your LLM will either:\n",
        "- Return an empty or fallback answer (✅ better than hallucinating!).\n",
        "- Or hallucinate — if you haven’t prompted it to stick to context only.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ How We Closed the Gap\n",
        "\n",
        "1️⃣ We located the missing Gainesville-specific page on our website.\n",
        "\n",
        "2️⃣ We added clear **Markdown headings** for each local indicator.\n",
        "\n",
        "3️⃣ We expanded bullet points with short, clear explanations to help our embedding model capture more semantic meaning.\n",
        "\n",
        "4️⃣ We saved the updated version as a **Markdown file** in our `/content` folder.\n",
        "- Using Markdown headings ensures each chunk has a clear semantic anchor when we split it for embeddings.\n",
        "- This makes the retriever more likely to match user queries to the right chunks.\n",
        "\n",
        "---\n",
        "\n",
        "### ✨ Why This Matters\n",
        "\n",
        "- Filling the gap improves **Recall@k** — the retriever can now find and return relevant local data.\n",
        "- Using clear headings and explanations improves **Precision@k** — chunks are more focused and less noisy.\n",
        "- Our pipeline stays trustworthy: no hallucinations, and better coverage of real customer questions.\n",
        "\n",
        "---\n",
        "\n",
        "**Key takeaway:**  \n",
        "Regularly test real questions, look for fallback answers, and add missing high-value content — *optimized with good structure* — to strengthen your RAG system over time!\n",
        "\n"
      ],
      "metadata": {
        "id": "_-Hqzih_cNTl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## ✅ **Key Next Steps**\n",
        "\n",
        "### **1️⃣ Add Retrieval Metrics**\n",
        "\n",
        "You want to test: **Did the retriever return the “right” chunks for the question?**\n",
        "\n",
        "🔑 **What you need:**\n",
        "\n",
        "* A small **ground truth mapping**: For each test question, note what chunk(s) should be retrieved.\n",
        "* Evaluate metrics like **Recall\\@k**, **Precision\\@k**, and **F1\\@k** for retrieval.\n",
        "\n",
        "📌 **How to do it:**\n",
        "\n",
        "* Add an “expected\\_chunks” field to your `qa_test` list (or store in a CSV).\n",
        "* When you retrieve `docs` for a question, compare the returned chunk IDs to the expected ones.\n",
        "* Count matches:\n",
        "\n",
        "  * **Recall\\@k** = (# relevant chunks retrieved) / (# relevant chunks in ground truth)\n",
        "  * **Precision\\@k** = (# relevant chunks retrieved) / (k)\n",
        "\n",
        "✅ Example:\n",
        "\n",
        "```python\n",
        "def evaluate_retrieval(retrieved_docs, expected_chunk_ids):\n",
        "    retrieved_ids = [doc.metadata[\"source\"] for doc in retrieved_docs]\n",
        "    relevant = set(expected_chunk_ids)\n",
        "    retrieved = set(retrieved_ids)\n",
        "\n",
        "    true_positives = len(retrieved & relevant)\n",
        "    recall = true_positives / len(relevant)\n",
        "    precision = true_positives / len(retrieved)\n",
        "\n",
        "    return recall, precision\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **2️⃣ Automate End-to-End QA Loop**\n",
        "\n",
        "You already loop through test questions. Enhance this by:\n",
        "\n",
        "* Storing each: question, retrieved docs, generated answer, context, retriever metrics, generation acceptability, and feedback.\n",
        "* Save results to a CSV or Pandas DataFrame for easy review.\n",
        "\n",
        "---\n",
        "\n",
        "### **3️⃣ Add `RAGAS` or `LlamaIndex` for Reference**\n",
        "\n",
        "You could replicate this manually (which you’re halfway doing), or use **RAGAS**:\n",
        "\n",
        "* It uses LLMs to judge **Faithfulness**, **Answer Relevancy**, and **Context Recall**.\n",
        "* It’s basically what you wrote — just more automated.\n",
        "\n",
        "---\n",
        "\n",
        "### **4️⃣ Track Failure Cases**\n",
        "\n",
        "When a question scores low on retrieval or generation, log:\n",
        "\n",
        "* Which ground truth chunks were missed?\n",
        "* Why did the LLM hallucinate? Was the context incomplete?\n",
        "* Use this to refine chunk size, overlap, or even the prompt.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **5️⃣ Continuous Improvement**\n",
        "\n",
        "* Keep a growing test set of real user questions.\n",
        "* Automate logging and store failed cases.\n",
        "* Fine-tune your chunking and retriever based on low Recall\\@k or low faithfulness.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Here’s What We’ll Do with RAGAS\n",
        "\n",
        "RAGAS can automatically score:\n",
        "\n",
        "* **Context Recall** (similar to Recall\\@k)\n",
        "* **Faithfulness** (how much the generated answer sticks to the retrieved context)\n",
        "* **Answer Relevance** (how well the answer matches the question)\n",
        "* **Answer Correctness** (if you have ground truth answers — optional but best)\n",
        "\n",
        "You already have:\n",
        "\n",
        "* A retriever that returns chunks.\n",
        "* A generator that returns the answer + context.\n",
        "* An evaluator loop — which we’ll replace or extend with `ragas`.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Questions for You (to plan this right)\n",
        "\n",
        "1️⃣ **Do you have ground truth answers?**\n",
        "RAGAS’s `Answer Correctness` metric compares the generated answer to a known good answer.\n",
        "\n",
        "* If you don’t have these yet, we can use only `Context Recall` + `Faithfulness` + `Answer Relevance`.\n",
        "\n",
        "2️⃣ **Do you want to evaluate on a small test set for now?**\n",
        "Let’s start with 5–10 examples so you don’t burn credits or wait too long.\n",
        "\n",
        "3️⃣ **Are you okay with using OpenAI as the backend LLM for the RAGAS eval?**\n",
        "It needs an LLM to do the judgment, same as your custom evaluator — you just need your `OPENAI_API_KEY` set up.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Next Steps (High-Level)\n",
        "\n",
        "Here’s the flow for your Colab:\n",
        "\n",
        "```plaintext\n",
        "1. Install or update RAGAS and dependencies.\n",
        "2. Create a Dataset:\n",
        "   Each row = { question, answer, contexts, (optional) ground_truth }\n",
        "3. Run RAGAS pipeline:\n",
        "   - ragas.evaluate(dataset, metrics=[...])\n",
        "4. Inspect results: see scores for each metric.\n",
        "5. Save to CSV or visualize.\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Po3HNAFe9f7X"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xAgY_zUk-pqE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}